{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4d914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s1gm0id/educative/rag-langchain-gemini/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai \n",
    "from IPython.display import display \n",
    "from IPython.display import Markdown \n",
    "import textwrap \n",
    "from langchain_community.document_loaders import PyPDFLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI \n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain.chains import RetrievalQA \n",
    "\n",
    "genai.configure(api_key='<YOUR_API_KEY>') \n",
    "model = genai.GenerativeModel('gemini-2.5-flash-lite') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740c6af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an explanation of AI in three bullet points:\n",
      "\n",
      "*   **AI involves creating computer systems that can perform tasks typically requiring human intelligence.** This includes things like learning from experience, problem-solving, making decisions, understanding language, and recognizing patterns in data.\n",
      "\n",
      "*   **It's driven by algorithms and vast amounts of data.** AI systems are trained on massive datasets to identify relationships and patterns, allowing them to make predictions or take actions without being explicitly programmed for every single scenario.\n",
      "\n",
      "*   **AI powers a wide range of applications, from everyday conveniences to complex scientific research.** Examples include virtual assistants (like Siri or Alexa), recommendation engines (on Netflix or Amazon), self-driving cars, medical diagnosis tools, and sophisticated financial trading systems.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's an explanation of AI in three bullet points:\n",
       "\n",
       "*   **AI involves creating computer systems that can perform tasks typically requiring human intelligence.** This includes things like learning from experience, problem-solving, making decisions, understanding language, and recognizing patterns in data.\n",
       "\n",
       "*   **It's driven by algorithms and vast amounts of data.** AI systems are trained on massive datasets to identify relationships and patterns, allowing them to make predictions or take actions without being explicitly programmed for every single scenario.\n",
       "\n",
       "*   **AI powers a wide range of applications, from everyday conveniences to complex scientific research.** Examples include virtual assistants (like Siri or Alexa), recommendation engines (on Netflix or Amazon), self-driving cars, medical diagnosis tools, and sophisticated financial trading systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask questions using prompts \n",
    "response = model.generate_content(\"Explain AI with three bullet points.\") \n",
    "print(response.text) \n",
    "Markdown(response.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d97c4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Classic Aglio Olio Pasta: A Simple Italian Delight\n",
       "\n",
       "Aglio Olio (pronounced AH-lyee-o OH-lyo) is a quintessential Italian pasta dish that's as delicious as it is simple. It relies on just a few high-quality ingredients for its incredible flavor. Here's a recipe for you to recreate this classic at home:\n",
       "\n",
       "**Yields:** 2 servings\n",
       "**Prep time:** 5 minutes\n",
       "**Cook time:** 15-20 minutes\n",
       "\n",
       "**Ingredients:**\n",
       "\n",
       "*   **200g (7 oz) dried spaghetti or linguine:** Thicker strands hold up well to the sauce.\n",
       "*   **4-6 cloves garlic:** Adjust to your garlic preference! Thinly sliced is best.\n",
       "*   **1/4 - 1/2 teaspoon red pepper flakes (chili flakes):** Again, adjust to your heat tolerance.\n",
       "*   **60ml (1/4 cup) extra virgin olive oil:** Use a good quality olive oil as it's a primary flavor component.\n",
       "*   **1/4 cup chopped fresh parsley:** Fresh parsley is essential for brightness.\n",
       "*   **Salt to taste:** For seasoning the pasta water and the sauce.\n",
       "*   **Freshly ground black pepper to taste:** For finishing.\n",
       "*   **(Optional) Grated Parmesan cheese:** For serving, though traditionally not included in pure Aglio Olio.\n",
       "\n",
       "**Equipment:**\n",
       "\n",
       "*   Large pot for boiling pasta\n",
       "*   Large skillet or frying pan\n",
       "\n",
       "**Instructions:**\n",
       "\n",
       "1.  **Cook the Pasta:**\n",
       "    *   Fill a large pot with plenty of water. Add a generous amount of salt (it should taste like the sea).\n",
       "    *   Bring the water to a rolling boil over high heat.\n",
       "    *   Add the spaghetti and cook according to package directions until al dente (tender but still firm to the bite).\n",
       "    *   **Crucially, before draining the pasta, reserve about 1-1.5 cups of the starchy pasta water.** This is your secret ingredient for creating a silky sauce.\n",
       "\n",
       "2.  **Prepare the Garlic and Chili:**\n",
       "    *   While the pasta is cooking, thinly slice the garlic cloves.\n",
       "    *   Have your red pepper flakes ready.\n",
       "\n",
       "3.  **Infuse the Olive Oil:**\n",
       "    *   In a large skillet, heat the extra virgin olive oil over **low to medium-low heat.** This is key to infusing the oil without burning the garlic.\n",
       "    *   Add the sliced garlic to the cool or barely warm olive oil. This allows the garlic flavor to slowly release into the oil.\n",
       "    *   Cook the garlic gently, stirring occasionally, until it's fragrant and just beginning to turn a very pale golden color. **Do not let it brown or burn, as this will make it bitter.** If it starts to color too quickly, reduce the heat further or temporarily remove the pan from the heat.\n",
       "    *   Add the red pepper flakes to the oil and stir for about 30 seconds, allowing them to bloom and release their heat.\n",
       "\n",
       "4.  **Combine and Emulsify:**\n",
       "    *   Once the pasta is al dente, drain it well.\n",
       "    *   Add the drained pasta directly to the skillet with the garlic-infused olive oil.\n",
       "    *   Add about 1/2 cup of the reserved pasta water to the skillet.\n",
       "    *   Increase the heat to medium and toss the pasta vigorously with tongs. The hot pasta, oil, and starchy water will emulsify to create a light, glossy sauce that coats the spaghetti.\n",
       "    *   Continue tossing and adding more pasta water a tablespoon at a time, if needed, until the sauce reaches your desired consistency. It should be glossy and not watery.\n",
       "\n",
       "5.  **Finish and Serve:**\n",
       "    *   Stir in the chopped fresh parsley.\n",
       "    *   Season generously with salt and freshly ground black pepper to taste. Remember, the pasta water was salted, so taste before adding too much more salt.\n",
       "    *   Toss everything together once more.\n",
       "    *   Serve immediately, divided between two plates.\n",
       "\n",
       "**Tips for Perfection:**\n",
       "\n",
       "*   **Garlic is King:** Don't be shy with the garlic! Thinly sliced garlic infuses the oil better than minced garlic, which can burn more easily.\n",
       "*   **Gentle Heat for Garlic:** The most common mistake is burning the garlic. Keep the heat low and slow to achieve a sweet, mellow garlic flavor.\n",
       "*   **Pasta Water is Your Friend:** The starch in the pasta water is what creates the emulsion and gives the sauce its body. Don't skip reserving it!\n",
       "*   **Quality Ingredients:** Since there are so few ingredients, the quality of your olive oil and pasta really shines through.\n",
       "*   **Fresh Parsley is a Must:** Dried parsley won't give you the same fresh, bright flavor.\n",
       "*   **Don't Overcrowd the Pan:** Use a large enough skillet so you can toss the pasta easily without it sticking together.\n",
       "\n",
       "Enjoy your delicious and simple Aglio Olio pasta! Buon appetito!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chatting with Gemini and retrieving chat history \n",
    "history = model.start_chat() \n",
    "\n",
    "response = history.send_message(\"Give me a recipe for Aglio Olio pasta\") \n",
    "Markdown(response.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cc77c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts {\n",
      "  text: \"Give me a recipe for Aglio Olio pasta\"\n",
      "}\n",
      "role: \"user\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "parts {\n",
      "  text: \"## Classic Aglio Olio Pasta: A Simple Italian Delight\\n\\nAglio Olio (pronounced AH-lyee-o OH-lyo) is a quintessential Italian pasta dish that\\'s as delicious as it is simple. It relies on just a few high-quality ingredients for its incredible flavor. Here\\'s a recipe for you to recreate this classic at home:\\n\\n**Yields:** 2 servings\\n**Prep time:** 5 minutes\\n**Cook time:** 15-20 minutes\\n\\n**Ingredients:**\\n\\n*   **200g (7 oz) dried spaghetti or linguine:** Thicker strands hold up well to the sauce.\\n*   **4-6 cloves garlic:** Adjust to your garlic preference! Thinly sliced is best.\\n*   **1/4 - 1/2 teaspoon red pepper flakes (chili flakes):** Again, adjust to your heat tolerance.\\n*   **60ml (1/4 cup) extra virgin olive oil:** Use a good quality olive oil as it\\'s a primary flavor component.\\n*   **1/4 cup chopped fresh parsley:** Fresh parsley is essential for brightness.\\n*   **Salt to taste:** For seasoning the pasta water and the sauce.\\n*   **Freshly ground black pepper to taste:** For finishing.\\n*   **(Optional) Grated Parmesan cheese:** For serving, though traditionally not included in pure Aglio Olio.\\n\\n**Equipment:**\\n\\n*   Large pot for boiling pasta\\n*   Large skillet or frying pan\\n\\n**Instructions:**\\n\\n1.  **Cook the Pasta:**\\n    *   Fill a large pot with plenty of water. Add a generous amount of salt (it should taste like the sea).\\n    *   Bring the water to a rolling boil over high heat.\\n    *   Add the spaghetti and cook according to package directions until al dente (tender but still firm to the bite).\\n    *   **Crucially, before draining the pasta, reserve about 1-1.5 cups of the starchy pasta water.** This is your secret ingredient for creating a silky sauce.\\n\\n2.  **Prepare the Garlic and Chili:**\\n    *   While the pasta is cooking, thinly slice the garlic cloves.\\n    *   Have your red pepper flakes ready.\\n\\n3.  **Infuse the Olive Oil:**\\n    *   In a large skillet, heat the extra virgin olive oil over **low to medium-low heat.** This is key to infusing the oil without burning the garlic.\\n    *   Add the sliced garlic to the cool or barely warm olive oil. This allows the garlic flavor to slowly release into the oil.\\n    *   Cook the garlic gently, stirring occasionally, until it\\'s fragrant and just beginning to turn a very pale golden color. **Do not let it brown or burn, as this will make it bitter.** If it starts to color too quickly, reduce the heat further or temporarily remove the pan from the heat.\\n    *   Add the red pepper flakes to the oil and stir for about 30 seconds, allowing them to bloom and release their heat.\\n\\n4.  **Combine and Emulsify:**\\n    *   Once the pasta is al dente, drain it well.\\n    *   Add the drained pasta directly to the skillet with the garlic-infused olive oil.\\n    *   Add about 1/2 cup of the reserved pasta water to the skillet.\\n    *   Increase the heat to medium and toss the pasta vigorously with tongs. The hot pasta, oil, and starchy water will emulsify to create a light, glossy sauce that coats the spaghetti.\\n    *   Continue tossing and adding more pasta water a tablespoon at a time, if needed, until the sauce reaches your desired consistency. It should be glossy and not watery.\\n\\n5.  **Finish and Serve:**\\n    *   Stir in the chopped fresh parsley.\\n    *   Season generously with salt and freshly ground black pepper to taste. Remember, the pasta water was salted, so taste before adding too much more salt.\\n    *   Toss everything together once more.\\n    *   Serve immediately, divided between two plates.\\n\\n**Tips for Perfection:**\\n\\n*   **Garlic is King:** Don\\'t be shy with the garlic! Thinly sliced garlic infuses the oil better than minced garlic, which can burn more easily.\\n*   **Gentle Heat for Garlic:** The most common mistake is burning the garlic. Keep the heat low and slow to achieve a sweet, mellow garlic flavor.\\n*   **Pasta Water is Your Friend:** The starch in the pasta water is what creates the emulsion and gives the sauce its body. Don\\'t skip reserving it!\\n*   **Quality Ingredients:** Since there are so few ingredients, the quality of your olive oil and pasta really shines through.\\n*   **Fresh Parsley is a Must:** Dried parsley won\\'t give you the same fresh, bright flavor.\\n*   **Don\\'t Overcrowd the Pan:** Use a large enough skillet so you can toss the pasta easily without it sticking together.\\n\\nEnjoy your delicious and simple Aglio Olio pasta! Buon appetito!\"\n",
      "}\n",
      "role: \"model\"\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"## Classic Aglio Olio Pasta: A Simple Italian Delight\\n\\nAglio Olio (pronounced AH-lyee-o OH-lyo) is a quintessential Italian pasta dish that's as delicious as it is simple. It relies on just a few high-quality ingredients for its incredible flavor. Here's a recipe for you to recreate this classic at home:\\n\\n**Yields:** 2 servings\\n**Prep time:** 5 minutes\\n**Cook time:** 15-20 minutes\\n\\n**Ingredients:**\\n\\n*   **200g (7 oz) dried spaghetti or linguine:** Thicker strands hold up well to the sauce.\\n*   **4-6 cloves garlic:** Adjust to your garlic preference! Thinly sliced is best.\\n*   **1/4 - 1/2 teaspoon red pepper flakes (chili flakes):** Again, adjust to your heat tolerance.\\n*   **60ml (1/4 cup) extra virgin olive oil:** Use a good quality olive oil as it's a primary flavor component.\\n*   **1/4 cup chopped fresh parsley:** Fresh parsley is essential for brightness.\\n*   **Salt to taste:** For seasoning the pasta water and the sauce.\\n*   **Freshly ground black pepper to taste:** For finishing.\\n*   **(Optional) Grated Parmesan cheese:** For serving, though traditionally not included in pure Aglio Olio.\\n\\n**Equipment:**\\n\\n*   Large pot for boiling pasta\\n*   Large skillet or frying pan\\n\\n**Instructions:**\\n\\n1.  **Cook the Pasta:**\\n    *   Fill a large pot with plenty of water. Add a generous amount of salt (it should taste like the sea).\\n    *   Bring the water to a rolling boil over high heat.\\n    *   Add the spaghetti and cook according to package directions until al dente (tender but still firm to the bite).\\n    *   **Crucially, before draining the pasta, reserve about 1-1.5 cups of the starchy pasta water.** This is your secret ingredient for creating a silky sauce.\\n\\n2.  **Prepare the Garlic and Chili:**\\n    *   While the pasta is cooking, thinly slice the garlic cloves.\\n    *   Have your red pepper flakes ready.\\n\\n3.  **Infuse the Olive Oil:**\\n    *   In a large skillet, heat the extra virgin olive oil over **low to medium-low heat.** This is key to infusing the oil without burning the garlic.\\n    *   Add the sliced garlic to the cool or barely warm olive oil. This allows the garlic flavor to slowly release into the oil.\\n    *   Cook the garlic gently, stirring occasionally, until it's fragrant and just beginning to turn a very pale golden color. **Do not let it brown or burn, as this will make it bitter.** If it starts to color too quickly, reduce the heat further or temporarily remove the pan from the heat.\\n    *   Add the red pepper flakes to the oil and stir for about 30 seconds, allowing them to bloom and release their heat.\\n\\n4.  **Combine and Emulsify:**\\n    *   Once the pasta is al dente, drain it well.\\n    *   Add the drained pasta directly to the skillet with the garlic-infused olive oil.\\n    *   Add about 1/2 cup of the reserved pasta water to the skillet.\\n    *   Increase the heat to medium and toss the pasta vigorously with tongs. The hot pasta, oil, and starchy water will emulsify to create a light, glossy sauce that coats the spaghetti.\\n    *   Continue tossing and adding more pasta water a tablespoon at a time, if needed, until the sauce reaches your desired consistency. It should be glossy and not watery.\\n\\n5.  **Finish and Serve:**\\n    *   Stir in the chopped fresh parsley.\\n    *   Season generously with salt and freshly ground black pepper to taste. Remember, the pasta water was salted, so taste before adding too much more salt.\\n    *   Toss everything together once more.\\n    *   Serve immediately, divided between two plates.\\n\\n**Tips for Perfection:**\\n\\n*   **Garlic is King:** Don't be shy with the garlic! Thinly sliced garlic infuses the oil better than minced garlic, which can burn more easily.\\n*   **Gentle Heat for Garlic:** The most common mistake is burning the garlic. Keep the heat low and slow to achieve a sweet, mellow garlic flavor.\\n*   **Pasta Water is Your Friend:** The starch in the pasta water is what creates the emulsion and gives the sauce its body. Don't skip reserving it!\\n*   **Quality Ingredients:** Since there are so few ingredients, the quality of your olive oil and pasta really shines through.\\n*   **Fresh Parsley is a Must:** Dried parsley won't give you the same fresh, bright flavor.\\n*   **Don't Overcrowd the Pan:** Use a large enough skillet so you can toss the pasta easily without it sticking together.\\n\\nEnjoy your delicious and simple Aglio Olio pasta! Buon appetito!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop through history \n",
    "for i in history.history: \n",
    "    print(i) \n",
    "    print('\\n\\n') \n",
    "i.parts[0].text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ec428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_tokens: 14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also count the tokens\n",
    "model.count_tokens(\"Help me find the nearest supermarket from where I can buy the ingredients.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6659eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with 'temperature' parameter \n",
    "def get_response(prompt, generation_config={}): \n",
    "    response = model.generate_content(\n",
    "        contents=prompt, \n",
    "        generation_config=generation_config\n",
    "    ) \n",
    "    return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "104fe2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For temperature 0.0, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's break down XGBoost, a powerful and widely used machine learning algorithm, and then explore its real-life applications.\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "XGBoost stands for **eXtreme Gradient Boosting**. At its core, it's an implementation of the **gradient boosting** algorithm, but with significant optimizations and enhancements that make it incredibly fast and accurate.\n",
       "\n",
       "To understand XGBoost, let's first understand its building blocks:\n",
       "\n",
       "1.  **Ensemble Learning:** XGBoost is an ensemble method. This means it combines multiple \"weak\" learners (typically decision trees) to create a single, strong learner. The idea is that by combining many simple models, you can achieve better performance than any single model could on its own.\n",
       "\n",
       "2.  **Boosting:** Boosting is a specific type of ensemble learning where models are built sequentially. Each new model tries to correct the errors made by the previous models. It's like a team of students learning a subject: the first student learns, the second student learns from the first student's mistakes, and so on.\n",
       "\n",
       "3.  **Gradient Boosting:** This is where the \"gradient\" in XGBoost comes in. Gradient boosting works by minimizing a loss function (which measures how bad your model's predictions are). It does this by iteratively adding new models that are trained to predict the *residuals* (the errors) of the previous models. The \"gradient\" refers to the direction of steepest descent in the loss function, guiding the process of error correction.\n",
       "\n",
       "4.  **XGBoost's \"eXtreme\" Enhancements:** This is what sets XGBoost apart. It takes the core gradient boosting idea and makes it \"extreme\" through several key innovations:\n",
       "\n",
       "    *   **Regularization:** XGBoost incorporates L1 (Lasso) and L2 (Ridge) regularization directly into the objective function. This helps prevent overfitting by penalizing complex models, leading to better generalization on unseen data.\n",
       "    *   **Tree Pruning:** Unlike traditional gradient boosting that might grow trees to their full depth and then prune, XGBoost uses a \"depth-first\" approach and prunes trees based on a \"gain\" threshold. If splitting a node doesn't improve the objective function by a certain amount, it stops splitting.\n",
       "    *   **Handling Missing Values:** XGBoost has a built-in mechanism to handle missing values in the data. It learns the best direction to send instances with missing values during the tree splitting process.\n",
       "    *   **Parallel Processing:** XGBoost can leverage multiple CPU cores to speed up training. It can parallelize the process of finding the best split points for each tree.\n",
       "    *   **Cache-Aware Access:** It's designed to efficiently utilize CPU cache, further boosting performance.\n",
       "    *   **Out-of-Core Computation:** For datasets that don't fit into memory, XGBoost can handle them by processing data in chunks.\n",
       "    *   **Cross-Validation:** XGBoost has a built-in cross-validation function, making it easier to tune hyperparameters and assess model performance.\n",
       "\n",
       "**In essence, XGBoost is a highly optimized and regularized gradient boosting algorithm that is known for its speed, accuracy, and robustness.**\n",
       "\n",
       "## How it Works (Simplified Analogy)\n",
       "\n",
       "Imagine you're trying to predict the price of a house.\n",
       "\n",
       "1.  **Initial Guess:** You start with a very simple model, maybe just predicting the average price of all houses. This is your first \"weak learner.\"\n",
       "2.  **Identify Errors:** You look at the houses where your initial guess was wrong (the residuals). Some houses were predicted too low, others too high.\n",
       "3.  **Build a Correction Model:** You train a new, slightly more complex model (another decision tree) to predict these errors. For example, if your initial guess was too low for houses with many bedrooms, this new model will learn to add a \"bonus\" for more bedrooms.\n",
       "4.  **Combine and Refine:** You combine your initial guess with the predictions of the correction model. This gives you a better overall prediction.\n",
       "5.  **Repeat:** You repeat steps 2-4 many times. Each new model focuses on correcting the remaining errors of the combined previous models.\n",
       "6.  **XGBoost's Magic:** XGBoost does this very efficiently. It uses regularization to ensure the correction models don't become too complex and overfit. It also uses clever techniques to speed up the process of finding the best ways to correct errors.\n",
       "\n",
       "## Real-Life Use Cases of XGBoost\n",
       "\n",
       "XGBoost's power and versatility have made it a go-to algorithm for a wide range of problems across various industries. Here are some prominent real-life use cases:\n",
       "\n",
       "### 1. Finance\n",
       "\n",
       "*   **Credit Risk Assessment:** Predicting the likelihood of a borrower defaulting on a loan. XGBoost can analyze a vast number of features (income, credit history, loan amount, etc.) to build highly accurate risk models.\n",
       "    *   **Example:** Banks use XGBoost to decide whether to approve loan applications and at what interest rate.\n",
       "*   **Fraud Detection:** Identifying fraudulent transactions in credit card usage, insurance claims, or online activities. XGBoost can detect subtle patterns that might indicate fraudulent behavior.\n",
       "    *   **Example:** E-commerce platforms use XGBoost to flag suspicious transactions in real-time.\n",
       "*   **Algorithmic Trading:** Developing strategies for buying and selling financial assets based on market data. XGBoost can predict stock price movements or identify trading opportunities.\n",
       "    *   **Example:** Hedge funds use XGBoost to build predictive models for trading strategies.\n",
       "*   **Customer Churn Prediction:** Identifying customers who are likely to stop using a service. This allows companies to proactively offer incentives or address issues to retain them.\n",
       "    *   **Example:** Telecom companies use XGBoost to predict which customers are likely to switch providers.\n",
       "\n",
       "### 2. E-commerce and Retail\n",
       "\n",
       "*   **Recommendation Systems:** Suggesting products or content to users based on their past behavior and preferences. XGBoost can power sophisticated recommendation engines.\n",
       "    *   **Example:** Online retailers like Amazon use XGBoost to recommend products you might like.\n",
       "*   **Sales Forecasting:** Predicting future sales volumes for products or stores. This helps with inventory management, staffing, and marketing.\n",
       "    *   **Example:** Retailers use XGBoost to forecast demand for seasonal products.\n",
       "*   **Customer Lifetime Value (CLV) Prediction:** Estimating the total revenue a customer is expected to generate over their relationship with a company.\n",
       "    *   **Example:** Marketing teams use CLV predictions to allocate marketing budgets effectively.\n",
       "\n",
       "### 3. Healthcare\n",
       "\n",
       "*   **Disease Prediction and Diagnosis:** Identifying patients at risk of developing certain diseases or assisting in the diagnosis of medical conditions based on patient data.\n",
       "    *   **Example:** Researchers use XGBoost to predict the likelihood of a patient developing diabetes based on their medical history and lifestyle.\n",
       "*   **Drug Discovery:** Analyzing biological data to identify potential drug candidates or predict the efficacy of new treatments.\n",
       "    *   **Example:** Pharmaceutical companies use XGBoost to screen potential drug compounds.\n",
       "*   **Patient Readmission Prediction:** Identifying patients who are at high risk of being readmitted to the hospital shortly after discharge.\n",
       "    *   **Example:** Hospitals use XGBoost to identify patients who need extra post-discharge care.\n",
       "\n",
       "### 4. Technology and Internet\n",
       "\n",
       "*   **Search Engine Ranking:** Improving the relevance of search results by predicting which pages are most likely to satisfy a user's query.\n",
       "    *   **Example:** Google and other search engines use complex ranking algorithms that can incorporate gradient boosting techniques.\n",
       "*   **Ad Click-Through Rate (CTR) Prediction:** Estimating the probability that a user will click on an online advertisement. This is crucial for targeted advertising.\n",
       "    *   **Example:** Online advertising platforms use XGBoost to optimize ad placement and targeting.\n",
       "*   **Spam Detection:** Identifying and filtering out unwanted emails or messages.\n",
       "    *   **Example:** Email providers use XGBoost to classify emails as spam or not spam.\n",
       "\n",
       "### 5. Other Industries\n",
       "\n",
       "*   **Manufacturing:** Predictive maintenance, identifying potential equipment failures before they occur.\n",
       "    *   **Example:** Factories use XGBoost to predict when a machine is likely to break down, allowing for proactive maintenance.\n",
       "*   **Transportation:** Optimizing logistics, predicting traffic patterns, and improving route planning.\n",
       "    *   **Example:** Ride-sharing services use XGBoost to predict demand in different areas and optimize driver allocation.\n",
       "*   **Energy:** Predicting energy consumption, optimizing grid management, and forecasting renewable energy generation.\n",
       "    *   **Example:** Energy companies use XGBoost to predict electricity demand.\n",
       "\n",
       "## Why is XGBoost So Popular?\n",
       "\n",
       "*   **High Accuracy:** It consistently achieves state-of-the-art results on many tabular datasets.\n",
       "*   **Speed and Performance:** Its optimizations make it significantly faster than many other gradient boosting implementations.\n",
       "*   **Robustness:** Its regularization techniques and handling of missing values make it less prone to overfitting and more resilient to noisy data.\n",
       "*   **Flexibility:** It can be used for both classification and regression tasks.\n",
       "*   **Scalability:** It can handle large datasets efficiently.\n",
       "*   **Community Support:** It has a large and active community, meaning plenty of resources, tutorials, and support are available.\n",
       "\n",
       "In summary, XGBoost is a powerful and efficient machine learning algorithm that has revolutionized how we approach many predictive modeling tasks. Its ability to deliver high accuracy with speed and robustness makes it an invaluable tool for data scientists and businesses across a wide spectrum of applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For temperature 0.25, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's break down XGBoost, a powerful machine learning algorithm, and explore its real-world applications.\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "XGBoost stands for **eXtreme Gradient Boosting**. At its core, it's an implementation of the **gradient boosting** algorithm, but with significant optimizations and enhancements that make it incredibly fast and accurate.\n",
       "\n",
       "Here's a breakdown of the key concepts:\n",
       "\n",
       "1.  **Ensemble Learning:** XGBoost is an **ensemble learning** method. This means it combines multiple \"weak learners\" (typically decision trees) to create a single, strong predictive model. The idea is that by combining many simple models, you can achieve better performance than any single model could on its own.\n",
       "\n",
       "2.  **Gradient Boosting:** This is the core technique. Gradient boosting works iteratively:\n",
       "    *   **Start with a simple model:** Usually, this is a model that predicts the average of the target variable.\n",
       "    *   **Calculate residuals:** The difference between the actual values and the predictions of the current model are calculated. These are called \"residuals\" or \"errors.\"\n",
       "    *   **Train a new model to predict residuals:** The next model in the sequence is trained to predict these residuals. The goal is to correct the errors of the previous model.\n",
       "    *   **Add the new model's prediction:** The prediction of the new model is added to the predictions of the previous models, with a small learning rate (shrinkage) to prevent overfitting.\n",
       "    *   **Repeat:** This process is repeated for a specified number of iterations. Each new tree tries to fix the mistakes of the ensemble of trees built so far.\n",
       "\n",
       "3.  **XGBoost's Enhancements (Why it's \"eXtreme\"):** XGBoost takes gradient boosting to the next level with several key improvements:\n",
       "    *   **Regularization:** XGBoost incorporates L1 (Lasso) and L2 (Ridge) regularization directly into the objective function. This helps prevent overfitting by penalizing complex models.\n",
       "    *   **Tree Pruning:** Unlike traditional gradient boosting that might grow trees to full depth and then prune, XGBoost uses a \"pre-sorted\" approach and prunes trees based on a \"max_depth\" parameter and a \"gamma\" parameter (minimum loss reduction required to make a further partition on a leaf node).\n",
       "    *   **Handling Missing Values:** XGBoost has a built-in mechanism to handle missing values. It learns the best direction to go when a value is missing during tree traversal.\n",
       "    *   **Parallel Processing:** XGBoost can leverage multi-core processors to speed up training by parallelizing the tree building process.\n",
       "    *   **Cache-Aware Access:** It's designed to efficiently utilize hardware caches, further boosting performance.\n",
       "    *   **Out-of-Core Computation:** For very large datasets that don't fit into memory, XGBoost can perform computations out-of-core, using disk space.\n",
       "    *   **Weighted Quantile Sketch:** This is an efficient way to find the best split points for continuous features, even with large datasets.\n",
       "\n",
       "**In essence, XGBoost is a highly optimized and regularized gradient boosting framework that delivers exceptional predictive accuracy and speed.**\n",
       "\n",
       "## Real-Life Use Cases of XGBoost\n",
       "\n",
       "XGBoost has become a go-to algorithm for many data science competitions and real-world applications due to its robustness and performance. Here are some prominent examples:\n",
       "\n",
       "### 1. E-commerce and Retail:\n",
       "\n",
       "*   **Customer Churn Prediction:** Identifying customers who are likely to stop using a service or product. XGBoost can analyze customer behavior, purchase history, demographics, and engagement metrics to predict churn with high accuracy. This allows businesses to proactively offer incentives or personalized experiences to retain customers.\n",
       "    *   **Example:** An online streaming service uses XGBoost to predict which subscribers are likely to cancel their subscription based on viewing habits, subscription duration, and customer support interactions.\n",
       "*   **Product Recommendation Systems:** While not always the primary algorithm for collaborative filtering, XGBoost can be used to build sophisticated recommendation engines by incorporating a wide range of features about users and products.\n",
       "    *   **Example:** An e-commerce giant uses XGBoost to predict the probability of a customer purchasing a specific product based on their browsing history, past purchases, items in their cart, and the characteristics of the product itself.\n",
       "*   **Sales Forecasting:** Predicting future sales volumes for products or stores. XGBoost can consider historical sales data, seasonality, promotional activities, economic indicators, and even weather patterns.\n",
       "    *   **Example:** A retail chain uses XGBoost to forecast daily sales for each of its stores, helping with inventory management and staffing.\n",
       "\n",
       "### 2. Finance:\n",
       "\n",
       "*   **Credit Risk Assessment/Loan Default Prediction:** Predicting the likelihood of a borrower defaulting on a loan. XGBoost can analyze a vast array of financial data, including credit scores, income, debt-to-income ratio, employment history, and loan terms.\n",
       "    *   **Example:** A bank uses XGBoost to assess the creditworthiness of loan applicants, assigning a risk score to each applicant to decide whether to approve the loan and at what interest rate.\n",
       "*   **Fraud Detection:** Identifying fraudulent transactions in credit card usage, insurance claims, or online activities. XGBoost can learn patterns associated with fraudulent behavior from historical data.\n",
       "    *   **Example:** A credit card company uses XGBoost to flag suspicious transactions in real-time, preventing financial losses due to fraud.\n",
       "*   **Algorithmic Trading:** Developing trading strategies by predicting stock price movements or market trends. XGBoost can analyze historical market data, news sentiment, and economic indicators.\n",
       "    *   **Example:** A hedge fund uses XGBoost to predict the short-term price movements of stocks, informing their automated trading decisions.\n",
       "\n",
       "### 3. Healthcare:\n",
       "\n",
       "*   **Disease Prediction and Diagnosis:** Predicting the likelihood of a patient developing a specific disease or assisting in diagnosis. XGBoost can analyze patient medical history, genetic information, lifestyle factors, and diagnostic test results.\n",
       "    *   **Example:** A hospital uses XGBoost to predict the risk of a patient developing diabetes based on their age, BMI, family history, and blood test results.\n",
       "*   **Drug Discovery and Development:** Identifying potential drug candidates or predicting the efficacy of existing drugs.\n",
       "    *   **Example:** A pharmaceutical company uses XGBoost to predict the binding affinity of potential drug molecules to target proteins.\n",
       "*   **Patient Readmission Prediction:** Identifying patients who are at high risk of being readmitted to the hospital shortly after discharge.\n",
       "    *   **Example:** A hospital uses XGBoost to identify patients who might need extra support or follow-up care after leaving the hospital to reduce readmission rates.\n",
       "\n",
       "### 4. Technology and Internet:\n",
       "\n",
       "*   **Search Engine Ranking:** Improving the relevance of search results by predicting which pages are most likely to satisfy a user's query.\n",
       "    *   **Example:** A search engine uses XGBoost to rank web pages based on factors like keyword relevance, page authority, and user click-through rates.\n",
       "*   **Ad Click-Through Rate (CTR) Prediction:** Predicting the probability that a user will click on an advertisement. This is crucial for online advertising platforms.\n",
       "    *   **Example:** An online advertising platform uses XGBoost to predict the likelihood of a user clicking on a specific ad, optimizing ad placement and targeting.\n",
       "*   **Natural Language Processing (NLP):** While deep learning models are dominant in many NLP tasks, XGBoost can be effective for tasks like text classification, sentiment analysis, and named entity recognition when combined with appropriate feature engineering.\n",
       "    *   **Example:** A company uses XGBoost to classify customer reviews as positive, negative, or neutral.\n",
       "\n",
       "### 5. Other Industries:\n",
       "\n",
       "*   **Manufacturing:** Predictive maintenance (predicting when machinery is likely to fail) and quality control.\n",
       "*   **Energy:** Load forecasting (predicting electricity demand) and renewable energy output prediction.\n",
       "*   **Transportation:** Predicting traffic congestion and optimizing delivery routes.\n",
       "\n",
       "## Why is XGBoost so Popular?\n",
       "\n",
       "*   **High Accuracy:** It consistently achieves state-of-the-art results on many tabular datasets.\n",
       "*   **Speed and Performance:** Its optimized implementation makes it significantly faster than many other gradient boosting libraries.\n",
       "*   **Flexibility:** It can handle various types of data and is suitable for both classification and regression tasks.\n",
       "*   **Robustness:** Its regularization techniques help prevent overfitting, making it more reliable on unseen data.\n",
       "*   **Feature Importance:** XGBoost provides insights into which features are most important for making predictions, aiding in model interpretability.\n",
       "\n",
       "In summary, XGBoost is a powerful and versatile machine learning algorithm that has revolutionized how many data scientists approach predictive modeling. Its ability to handle complex relationships in data, coupled with its speed and robustness, makes it an invaluable tool across a wide spectrum of industries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For temperature 0.5, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's break down XGBoost, a powerful and widely used machine learning algorithm, and explore its real-life applications.\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "XGBoost stands for **eXtreme Gradient Boosting**. At its core, it's an implementation of the **gradient boosting** algorithm, which is a powerful ensemble method. Ensemble methods combine multiple weak learning models to create a strong, more accurate model.\n",
       "\n",
       "Here's a simplified breakdown of the key ideas behind XGBoost:\n",
       "\n",
       "1.  **Decision Trees as Base Learners:** XGBoost primarily uses decision trees as its building blocks. These are relatively simple models that partition data based on features.\n",
       "\n",
       "2.  **Boosting:** Unlike methods like Random Forests that build trees independently, boosting builds trees sequentially. Each new tree tries to correct the errors made by the previous trees.\n",
       "\n",
       "3.  **Gradient Descent:** The \"Gradient\" in XGBoost refers to its use of **gradient descent**. In this context, it means that each new tree is trained to minimize a \"loss function\" (which measures how bad the model's predictions are). The algorithm calculates the \"gradient\" of this loss function with respect to the predictions of the current ensemble and uses this gradient to guide the construction of the next tree. Essentially, it's trying to find the direction that most effectively reduces the errors.\n",
       "\n",
       "4.  **Regularization:** This is where \"eXtreme\" comes into play. XGBoost incorporates several **regularization techniques** to prevent overfitting, which is when a model learns the training data too well and performs poorly on unseen data. These include:\n",
       "    *   **L1 and L2 Regularization:** Penalizes large weights in the model, discouraging overly complex trees.\n",
       "    *   **Tree Pruning:** Limits the depth and complexity of individual trees.\n",
       "    *   **Column Subsampling:** Randomly samples features for each tree, similar to Random Forests.\n",
       "    *   **Row Subsampling:** Randomly samples data instances for each tree.\n",
       "\n",
       "5.  **Parallel Processing and Cache Awareness:** XGBoost is highly optimized for speed and efficiency. It can leverage multiple CPU cores for parallel processing and is designed to be cache-aware, meaning it efficiently utilizes computer memory.\n",
       "\n",
       "**In essence, XGBoost builds a sequence of decision trees, where each new tree focuses on improving the predictions of the combined previous trees by learning from their mistakes, while employing strong regularization to ensure generalization.**\n",
       "\n",
       "## Why is XGBoost So Popular?\n",
       "\n",
       "*   **High Accuracy:** It consistently achieves state-of-the-art results on many tabular data problems.\n",
       "*   **Speed and Performance:** Its optimized implementation makes it very fast, even on large datasets.\n",
       "*   **Flexibility:** It can handle various types of data and loss functions.\n",
       "*   **Robustness:** Its regularization techniques make it less prone to overfitting.\n",
       "*   **Feature Importance:** It provides insights into which features are most influential in making predictions.\n",
       "\n",
       "## Real-Life Use Cases of XGBoost\n",
       "\n",
       "XGBoost is a workhorse in many industries due to its power and versatility. Here are some prominent real-life use cases:\n",
       "\n",
       "### 1. Finance\n",
       "\n",
       "*   **Credit Risk Assessment:** Predicting the likelihood of a customer defaulting on a loan. XGBoost can analyze various financial and demographic factors to build highly accurate credit scoring models.\n",
       "    *   **Example:** A bank uses XGBoost to evaluate loan applications, identifying high-risk individuals to mitigate potential losses.\n",
       "*   **Fraud Detection:** Identifying fraudulent transactions in credit card usage, insurance claims, or online activities.\n",
       "    *   **Example:** An e-commerce platform uses XGBoost to flag suspicious transactions in real-time, preventing financial losses due to fraud.\n",
       "*   **Algorithmic Trading:** Developing strategies for automated trading in stock markets or other financial instruments.\n",
       "    *   **Example:** A hedge fund uses XGBoost to predict stock price movements based on historical data and market sentiment.\n",
       "*   **Customer Churn Prediction:** Identifying customers who are likely to stop using a service.\n",
       "    *   **Example:** A telecom company uses XGBoost to predict which customers are at risk of switching to a competitor and offers them targeted retention incentives.\n",
       "\n",
       "### 2. E-commerce and Retail\n",
       "\n",
       "*   **Recommendation Systems:** Suggesting products to customers based on their past behavior, preferences, and similar users.\n",
       "    *   **Example:** An online retailer uses XGBoost to power its \"Customers who bought this also bought...\" feature, increasing sales and customer engagement.\n",
       "*   **Sales Forecasting:** Predicting future sales volume for products or entire stores.\n",
       "    *   **Example:** A supermarket chain uses XGBoost to forecast demand for perishable goods, optimizing inventory management and reducing waste.\n",
       "*   **Customer Lifetime Value (CLV) Prediction:** Estimating the total revenue a customer is expected to generate over their relationship with a company.\n",
       "    *   **Example:** A subscription service uses XGBoost to identify high-value customers and tailor marketing campaigns to retain them.\n",
       "\n",
       "### 3. Healthcare\n",
       "\n",
       "*   **Disease Prediction and Diagnosis:** Identifying patients at risk of developing certain diseases or assisting in medical diagnoses.\n",
       "    *   **Example:** Researchers use XGBoost to analyze patient medical records, genetic data, and lifestyle factors to predict the likelihood of developing diabetes or heart disease.\n",
       "*   **Drug Discovery and Development:** Predicting the efficacy and potential side effects of new drugs.\n",
       "    *   **Example:** Pharmaceutical companies use XGBoost to screen potential drug candidates, accelerating the drug discovery process.\n",
       "*   **Patient Readmission Prediction:** Identifying patients who are likely to be readmitted to the hospital shortly after discharge.\n",
       "    *   **Example:** Hospitals use XGBoost to identify high-risk patients and provide them with targeted post-discharge care to reduce readmission rates.\n",
       "\n",
       "### 4. Marketing\n",
       "\n",
       "*   **Customer Segmentation:** Grouping customers into distinct segments based on their behavior and characteristics for targeted marketing campaigns.\n",
       "    *   **Example:** A marketing team uses XGBoost to segment their customer base and create personalized email campaigns for each segment.\n",
       "*   **Campaign Optimization:** Predicting the effectiveness of different marketing campaigns and allocating resources accordingly.\n",
       "    *   **Example:** An advertising agency uses XGBoost to predict which ad creatives are most likely to resonate with specific target audiences.\n",
       "\n",
       "### 5. Manufacturing\n",
       "\n",
       "*   **Predictive Maintenance:** Predicting when machinery is likely to fail, allowing for proactive maintenance and reducing downtime.\n",
       "    *   **Example:** A factory uses sensor data from its machines and XGBoost to predict equipment failures, scheduling maintenance before a breakdown occurs.\n",
       "*   **Quality Control:** Identifying defective products during the manufacturing process.\n",
       "    *   **Example:** An electronics manufacturer uses XGBoost to analyze sensor readings from the production line to detect and flag faulty components.\n",
       "\n",
       "### 6. Natural Language Processing (NLP) and Text Analysis\n",
       "\n",
       "*   **Sentiment Analysis:** Determining the emotional tone (positive, negative, neutral) of text data.\n",
       "    *   **Example:** A company uses XGBoost to analyze customer reviews and social media mentions to gauge public opinion about its products.\n",
       "*   **Spam Detection:** Identifying and filtering out spam emails or messages.\n",
       "    *   **Example:** Email providers use XGBoost to classify incoming emails as either legitimate or spam.\n",
       "\n",
       "### 7. Kaggle Competitions and Data Science Challenges\n",
       "\n",
       "XGBoost has been a dominant force in Kaggle competitions across various domains, demonstrating its ability to achieve top performance on diverse and challenging datasets. This has solidified its reputation as a go-to algorithm for data scientists.\n",
       "\n",
       "**In summary, XGBoost is a powerful and versatile algorithm that excels in predictive modeling tasks across a wide range of industries. Its ability to handle complex relationships in data, its speed, and its robustness make it an invaluable tool for solving real-world problems.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For temperature 0.75, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## XGBoost: The Powerhouse of Gradient Boosting\n",
       "\n",
       "XGBoost, short for **eXtreme Gradient Boosting**, is a highly efficient and powerful implementation of the gradient boosting algorithm. It's a machine learning technique that builds an ensemble of decision trees, where each new tree tries to correct the errors made by the previous ones. This iterative process of learning from mistakes makes XGBoost incredibly effective for a wide range of predictive modeling tasks.\n",
       "\n",
       "Let's break down the core concepts:\n",
       "\n",
       "**1. Ensemble Learning:**\n",
       "\n",
       "Imagine you're trying to predict something complex. Instead of relying on a single expert (a single decision tree), you gather a team of experts. Each expert has their own strengths and weaknesses. XGBoost is like that team. It combines the predictions of many individual decision trees to create a more robust and accurate overall prediction.\n",
       "\n",
       "**2. Boosting:**\n",
       "\n",
       "Boosting is a method of building an ensemble where models are added sequentially. Each new model focuses on improving the performance of the previous ones. In XGBoost, this is achieved by:\n",
       "\n",
       "*   **Weak Learners:** Each individual decision tree in XGBoost is typically a \"weak learner.\" This means it's not perfectly accurate on its own but can still make reasonable predictions.\n",
       "*   **Sequential Learning:** XGBoost builds trees one after another. The first tree makes a prediction. The second tree then focuses on the samples that the first tree got wrong (or had high error). This process continues, with each new tree trying to reduce the residual errors of the ensemble built so far.\n",
       "*   **Gradient Descent Analogy:** The \"gradient\" in XGBoost refers to the fact that it uses gradient descent optimization. Think of it like trying to find the lowest point in a valley. Each step you take is guided by the slope (gradient) of the terrain. In XGBoost, the \"terrain\" is the error function, and each new tree is a step that moves the model closer to minimizing that error.\n",
       "\n",
       "**3. Key Features that Make XGBoost \"Extreme\":**\n",
       "\n",
       "While gradient boosting itself is powerful, XGBoost adds several optimizations and features that make it stand out:\n",
       "\n",
       "*   **Regularization:** XGBoost incorporates L1 (Lasso) and L2 (Ridge) regularization techniques. These techniques penalize complex models by shrinking the weights of features, which helps prevent overfitting (where the model performs well on training data but poorly on unseen data).\n",
       "*   **Handling Missing Values:** XGBoost has a built-in mechanism to handle missing values in the data. It learns the best way to impute missing values during the tree-building process.\n",
       "*   **Parallel Processing:** XGBoost can efficiently utilize multiple CPU cores for parallel processing, significantly speeding up training times compared to traditional gradient boosting implementations.\n",
       "*   **Tree Pruning:** It employs advanced tree pruning techniques to prevent overfitting. It can stop growing a tree if it's no longer contributing positively to the overall accuracy.\n",
       "*   **Cache-Aware Access:** XGBoost is designed to efficiently utilize CPU cache, further optimizing its performance.\n",
       "*   **Cross-Validation:** It has built-in cross-validation capabilities, allowing users to easily tune hyperparameters and assess model performance without manual setup.\n",
       "\n",
       "**How XGBoost Works (Simplified):**\n",
       "\n",
       "1.  **Initial Prediction:** Start with a simple prediction (e.g., the average of the target variable).\n",
       "2.  **Calculate Residuals:** Calculate the difference between the actual values and the current prediction (these are the errors or residuals).\n",
       "3.  **Build a Tree:** Train a decision tree to predict these residuals. This new tree is designed to capture the patterns in the errors.\n",
       "4.  **Update Prediction:** Add the prediction of the new tree (scaled by a learning rate) to the current prediction. This refines the overall prediction.\n",
       "5.  **Repeat:** Repeat steps 2-4 many times, with each new tree focusing on correcting the remaining residuals.\n",
       "6.  **Final Prediction:** The final prediction is the sum of the initial prediction and the predictions of all the trees in the ensemble.\n",
       "\n",
       "---\n",
       "\n",
       "## Real-Life Use Cases of XGBoost:\n",
       "\n",
       "XGBoost's versatility and performance have made it a go-to algorithm for a wide array of machine learning applications. Here are some prominent real-life use cases:\n",
       "\n",
       "**1. E-commerce and Retail:**\n",
       "\n",
       "*   **Recommendation Systems:** Predicting which products a user is likely to buy based on their past behavior, demographics, and product similarities. XGBoost can handle the complexity of user preferences and product catalogs.\n",
       "*   **Customer Churn Prediction:** Identifying customers who are at risk of leaving a service or platform. XGBoost can analyze various customer interaction data to flag potential churners, allowing businesses to implement retention strategies.\n",
       "*   **Sales Forecasting:** Predicting future sales volumes for products or stores. This helps with inventory management, staffing, and marketing campaign planning.\n",
       "*   **Price Optimization:** Determining the optimal pricing for products to maximize revenue and profit.\n",
       "\n",
       "**Example:** An online retailer like **Amazon** might use XGBoost to predict which items a customer is most likely to add to their cart after viewing a specific product. They can also use it to predict which customers are likely to abandon their shopping carts.\n",
       "\n",
       "**2. Finance and Banking:**\n",
       "\n",
       "*   **Credit Scoring and Risk Assessment:** Evaluating the creditworthiness of loan applicants. XGBoost can analyze a vast number of financial and demographic factors to predict the probability of default.\n",
       "*   **Fraud Detection:** Identifying fraudulent transactions in credit card usage, insurance claims, or online payments. XGBoost can learn subtle patterns indicative of fraud that might be missed by simpler models.\n",
       "*   **Algorithmic Trading:** Developing trading strategies by predicting stock price movements or market trends.\n",
       "*   **Loan Default Prediction:** Predicting the likelihood of a borrower defaulting on a loan.\n",
       "\n",
       "**Example:** A **bank** could use XGBoost to assess the risk associated with a new loan application. By analyzing historical data of loan defaults and applicant profiles, XGBoost can provide a probability score, helping the bank make informed lending decisions.\n",
       "\n",
       "**3. Healthcare:**\n",
       "\n",
       "*   **Disease Prediction:** Predicting the likelihood of a patient developing a specific disease based on their medical history, genetic data, and lifestyle factors.\n",
       "*   **Drug Discovery and Development:** Predicting the efficacy and potential side effects of new drugs.\n",
       "*   **Patient Readmission Prediction:** Identifying patients who are at high risk of being readmitted to the hospital after discharge.\n",
       "*   **Medical Image Analysis:** While not its primary strength, XGBoost can be used in conjunction with feature extraction from medical images to aid in diagnosis.\n",
       "\n",
       "**Example:** A **hospital** could use XGBoost to predict which patients are at the highest risk of developing sepsis after surgery. This allows medical staff to monitor these patients more closely and intervene early, potentially saving lives.\n",
       "\n",
       "**4. Telecommunications:**\n",
       "\n",
       "*   **Customer Churn Prediction:** Similar to e-commerce, telecom companies use XGBoost to predict which subscribers are likely to switch to a competitor.\n",
       "*   **Network Optimization:** Predicting network traffic patterns to optimize resource allocation and prevent service disruptions.\n",
       "*   **Customer Lifetime Value (CLV) Prediction:** Estimating the total revenue a customer is expected to generate over their relationship with the company.\n",
       "\n",
       "**Example:** A **telecom provider** might use XGBoost to identify customers who are experiencing poor service or are unhappy with their current plan. By proactively reaching out to these customers with tailored offers, they can reduce churn.\n",
       "\n",
       "**5. Marketing and Advertising:**\n",
       "\n",
       "*   **Customer Segmentation:** Grouping customers into different segments based on their behavior and preferences for targeted marketing campaigns.\n",
       "*   **Ad Click-Through Rate (CTR) Prediction:** Predicting the probability that a user will click on a particular advertisement.\n",
       "*   **Campaign Performance Prediction:** Estimating the success of marketing campaigns before they are launched.\n",
       "\n",
       "**Example:** An **advertising platform** like Google Ads uses XGBoost to predict the likelihood of a user clicking on an ad based on their search queries, browsing history, and other contextual information. This helps optimize ad placement and maximize campaign ROI.\n",
       "\n",
       "**6. Natural Language Processing (NLP):**\n",
       "\n",
       "*   **Sentiment Analysis:** Determining the emotional tone (positive, negative, neutral) of text data, such as customer reviews or social media posts.\n",
       "*   **Text Classification:** Categorizing text documents into predefined classes (e.g., spam detection, topic classification).\n",
       "\n",
       "**Example:** A company analyzing **customer feedback** from surveys or social media can use XGBoost to automatically classify comments as positive or negative, providing valuable insights into customer satisfaction.\n",
       "\n",
       "**7. Cybersecurity:**\n",
       "\n",
       "*   **Intrusion Detection:** Identifying malicious activity or unauthorized access attempts on a network.\n",
       "*   **Malware Detection:** Classifying files or processes as malicious or benign.\n",
       "\n",
       "**Example:** A **cybersecurity firm** can use XGBoost to analyze network traffic logs and identify patterns that might indicate a cyberattack in progress, allowing for faster response and mitigation.\n",
       "\n",
       "**In summary, XGBoost is a powerful and versatile algorithm that has revolutionized many areas of machine learning due to its speed, accuracy, and ability to handle complex datasets. Its effectiveness in real-world applications stems from its robust architecture, advanced regularization techniques, and efficient implementation.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For temperature 1.0, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## XGBoost: A Supercharged Gradient Boosting Machine\n",
       "\n",
       "XGBoost (Extreme Gradient Boosting) is a highly efficient and powerful open-source implementation of the gradient boosting algorithm. It's designed for speed and performance, and has become a go-to choice for many data scientists and machine learning engineers for a wide range of predictive modeling tasks.\n",
       "\n",
       "**At its core, gradient boosting is an ensemble learning technique that builds a predictive model in a sequential manner.** It starts with a simple model (often a decision tree) and then iteratively adds new models that correct the errors of the previous ones. Think of it like a team of experts, where each new expert learns from the mistakes of the previous ones to improve the overall prediction.\n",
       "\n",
       "**What makes XGBoost \"Extreme\"?**\n",
       "\n",
       "XGBoost takes the core gradient boosting concept and optimizes it significantly in several key ways:\n",
       "\n",
       "* **Regularization:** XGBoost incorporates L1 (Lasso) and L2 (Ridge) regularization directly into its objective function. This helps prevent overfitting by penalizing complex models, leading to better generalization on unseen data.\n",
       "* **Handling Missing Values:** XGBoost has a built-in mechanism to handle missing values in the data. It learns the best direction to go when a value is missing, rather than simply imputing it.\n",
       "* **Parallel Processing:** XGBoost can leverage multi-core processors to build trees in parallel, significantly speeding up the training process, especially for large datasets.\n",
       "* **Tree Pruning:** Unlike traditional gradient boosting, XGBoost uses a more sophisticated tree pruning method. It grows trees to a maximum depth and then prunes back branches that don't contribute significantly to the model's accuracy.\n",
       "* **Cache-Aware Access:** XGBoost optimizes its data structures and algorithms to work efficiently with CPU caches, further improving performance.\n",
       "* **Out-of-Core Computation:** For very large datasets that don't fit into memory, XGBoost can handle computations out-of-core, breaking the data into chunks and processing them sequentially.\n",
       "* **Weighted Quantile Sketch:** This algorithm efficiently finds the best split points for features by considering the distribution of gradients, even on large datasets.\n",
       "\n",
       "**How it Works (Simplified Analogy):**\n",
       "\n",
       "Imagine you're trying to predict the price of a house.\n",
       "\n",
       "1. **First Expert (Simple Model):** An initial model might just look at the square footage and make a basic prediction. It will likely be wrong for many houses.\n",
       "2. **Second Expert (Correcting Errors):** The second expert focuses on the houses where the first model made the biggest mistakes. It learns how features like \"number of bedrooms\" or \"location\" contribute to those errors.\n",
       "3. **Third Expert (Further Refinement):** The third expert then focuses on the remaining errors, perhaps considering \"proximity to amenities\" or \"age of the house.\"\n",
       "4. **Continuing the Process:** This continues, with each new expert learning from the residual errors of the combined previous experts, until the overall prediction becomes very accurate.\n",
       "\n",
       "XGBoost is like having a highly efficient team of experts, each specializing in correcting the mistakes of the group, and doing it much faster and with less chance of overthinking (overfitting).\n",
       "\n",
       "## Real-Life Use Cases of XGBoost:\n",
       "\n",
       "XGBoost's power and versatility have made it a popular choice across various industries and applications. Here are some prominent real-life use cases:\n",
       "\n",
       "**1. E-commerce and Retail:**\n",
       "\n",
       "* **Customer Churn Prediction:** Identifying customers who are likely to stop using a service or making purchases. XGBoost can analyze historical customer behavior, demographics, and interaction data to predict churn risk.\n",
       "    * **Example:** An online streaming service uses XGBoost to predict which subscribers are likely to cancel their subscription based on their viewing habits, engagement levels, and customer support interactions.\n",
       "* **Recommendation Systems:** Suggesting products or content to users based on their past behavior and the behavior of similar users.\n",
       "    * **Example:** An e-commerce website uses XGBoost to recommend personalized product bundles to customers based on their browsing history, purchase patterns, and items they've added to their wishlist.\n",
       "* **Sales Forecasting:** Predicting future sales volumes for products or categories.\n",
       "    * **Example:** A retail chain uses XGBoost to forecast demand for different clothing items in specific stores, optimizing inventory management and reducing stockouts.\n",
       "* **Fraud Detection:** Identifying fraudulent transactions or activities.\n",
       "    * **Example:** A credit card company uses XGBoost to detect suspicious transactions by analyzing transaction amounts, locations, purchase history, and device information.\n",
       "\n",
       "**2. Finance:**\n",
       "\n",
       "* **Credit Risk Assessment:** Evaluating the likelihood of a borrower defaulting on a loan. XGBoost can analyze financial history, income, employment status, and other factors to predict creditworthiness.\n",
       "    * **Example:** A bank uses XGBoost to assess loan applications, assigning a risk score to each applicant to decide whether to approve the loan and at what interest rate.\n",
       "* **Algorithmic Trading:** Developing strategies for automated buying and selling of financial assets.\n",
       "    * **Example:** A hedge fund uses XGBoost to predict stock price movements, identifying profitable trading opportunities based on historical price data, news sentiment, and economic indicators.\n",
       "* **Fraud Detection (Financial):** As mentioned above, but specifically within the financial sector for things like money laundering.\n",
       "\n",
       "**3. Healthcare:**\n",
       "\n",
       "* **Disease Prediction and Diagnosis:** Identifying individuals at risk of developing certain diseases or assisting in the diagnosis of existing conditions.\n",
       "    * **Example:** A research institution uses XGBoost to predict the likelihood of a patient developing diabetes based on their genetic predispositions, lifestyle factors, and medical history.\n",
       "* **Drug Discovery and Development:** Predicting the efficacy and potential side effects of new drugs.\n",
       "    * **Example:** A pharmaceutical company uses XGBoost to predict which drug candidates are most likely to be effective against a specific disease, accelerating the drug discovery process.\n",
       "* **Patient Readmission Prediction:** Identifying patients who are at high risk of being readmitted to the hospital after discharge.\n",
       "    * **Example:** A hospital uses XGBoost to identify patients at high risk of readmission, allowing them to implement targeted post-discharge care and support to reduce readmission rates.\n",
       "\n",
       "**4. Technology and Internet:**\n",
       "\n",
       "* **Search Engine Ranking:** Optimizing the order of search results to provide the most relevant information to users.\n",
       "    * **Example:** A search engine uses XGBoost to rank web pages based on factors like keyword relevance, user engagement, and website authority.\n",
       "* **Ad Click-Through Rate (CTR) Prediction:** Estimating the probability that a user will click on an advertisement.\n",
       "    * **Example:** An online advertising platform uses XGBoost to predict the CTR of ads, enabling them to show more relevant ads to users and increase advertising revenue.\n",
       "* **Spam Detection:** Identifying and filtering out unwanted spam emails.\n",
       "    * **Example:** An email service provider uses XGBoost to classify incoming emails as spam or legitimate, protecting users from phishing and malicious content.\n",
       "\n",
       "**5. Other Industries:**\n",
       "\n",
       "* **Manufacturing:** Predicting equipment failures and optimizing maintenance schedules.\n",
       "    * **Example:** A manufacturing plant uses XGBoost to predict when a critical piece of machinery is likely to fail, allowing them to perform proactive maintenance and avoid costly downtime.\n",
       "* **Energy:** Forecasting energy demand and optimizing power generation.\n",
       "    * **Example:** An energy company uses XGBoost to predict electricity demand for the next day based on weather patterns, economic activity, and historical consumption data.\n",
       "* **Natural Language Processing (NLP):** Sentiment analysis, text classification, and named entity recognition.\n",
       "    * **Example:** A social media monitoring company uses XGBoost to analyze customer reviews and social media posts to gauge public sentiment towards a product or brand.\n",
       "\n",
       "**In summary, XGBoost is a powerful and versatile machine learning algorithm that excels in a wide range of predictive tasks. Its efficiency, accuracy, and ability to handle complex datasets have made it a cornerstone of modern data science and a valuable tool for businesses across numerous industries.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for temp in [0.0, 0.25, 0.5, 0.75, 1.0]: \n",
    "    config = genai.types.GenerationConfig(temperature=temp) \n",
    "    result = get_response(\"Explain the concept of XGBoost with real-life use cases.\", generation_config=config) \n",
    "\n",
    "    print(f\"\\n\\nFor temperature {temp}, result: \\n\\n\") \n",
    "    display(Markdown(result.text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7891e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For token value 1, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For token value 50, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's break down the concept of XGBoost and explore its real-life applications.\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "XGBoost (eXtreme Gradient Boosting) is a powerful and highly efficient open-source implementation of the gradient boosting"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For token value 100, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's dive into the world of XGBoost!\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "**XGBoost (Extreme Gradient Boosting)** is a powerful and highly efficient open-source library that implements the gradient boosting algorithm. It's renowned for its speed, performance, and flexibility, making it a go-to choice for many data science competitions and real-world applications.\n",
       "\n",
       "At its core, XGBoost is an **ensemble learning method**. This means it combines the predictions of multiple individual models"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For token value 150, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's dive into the concept of XGBoost and explore some real-life use cases.\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "XGBoost, which stands for **eXtreme Gradient Boosting**, is a powerful and highly efficient open-source implementation of the gradient boosting algorithm. It's renowned for its speed, performance, and ability to handle complex datasets.\n",
       "\n",
       "At its core, XGBoost is a **tree-based machine learning algorithm**. This means it builds an ensemble of decision trees, where each new tree tries to correct the errors made by the previous ones.\n",
       "\n",
       "Here's a breakdown of the key concepts:\n",
       "\n",
       "**1. Gradient Boosting:**\n",
       "\n",
       "*   **Ensemble Learning:** Instead of relying on a single model,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For token value 200, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## XGBoost: The \"Extreme Gradient Boosting\" Powerhouse\n",
       "\n",
       "XGBoost, short for **Extreme Gradient Boosting**, is a highly efficient and powerful machine learning algorithm that has gained immense popularity for its speed, accuracy, and robustness. It's a sophisticated implementation of the **gradient boosting** framework, which is itself a powerful ensemble learning technique.\n",
       "\n",
       "Let's break down the core concepts and then dive into real-life use cases.\n",
       "\n",
       "### Understanding the Core Concepts\n",
       "\n",
       "1.  **Ensemble Learning:** Instead of relying on a single model, ensemble learning combines multiple \"weak learners\" (usually decision trees) to create a stronger, more accurate model. Imagine a group of experts collaborating to solve a problem  each expert might have a niche strength, but together they can provide a more comprehensive and reliable solution.\n",
       "\n",
       "2.  **Gradient Boosting:** This is where the \"boosting\" comes in. Gradient boosting builds models sequentially, where each new model tries to correct the errors made by"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experiment with 'max_output_tokens' parameter \n",
    "for token in [1, 50, 100, 150, 200]: \n",
    "    config = genai.types.GenerationConfig(max_output_tokens=token) \n",
    "    result = get_response(\"Explain the concept of XGBoost with real-life use cases.\", generation_config=config) \n",
    "\n",
    "    print(f\"\\n\\nFor token value {token}, result: \\n\\n\") \n",
    "    display(Markdown(result.text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "269119ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top-k value 1, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's dive into the concept of XGBoost and explore its real-life applications.\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "XGBoost stands for **eXtreme Gradient Boosting**. It's a highly efficient, flexible, and portable implementation of gradient boosting algorithms. At its core, XGBoost is a **decision tree-based ensemble machine learning algorithm**.\n",
       "\n",
       "Here's a breakdown of the key concepts:\n",
       "\n",
       "1.  **Ensemble Learning:** Instead of relying on a single, complex model, XGBoost builds a \"committee\" of simpler models. In XGBoost's case, these are **decision trees**.\n",
       "\n",
       "2.  **Gradient Boosting:** This is the core principle. It's an iterative process where models are built sequentially.\n",
       "    *   The first model makes predictions.\n",
       "    *   The second model is trained to **correct the errors** (residuals) of the first model.\n",
       "    *   The third model corrects the errors of the combined predictions of the first two, and so on.\n",
       "    *   Each subsequent model focuses on the \"gradient\" of the loss function with respect to the predictions, hence \"gradient boosting.\"\n",
       "\n",
       "3.  **Decision Trees:** XGBoost uses **weak learners**, which are typically shallow decision trees. These trees are not very accurate on their own but, when combined, can achieve high accuracy.\n",
       "\n",
       "4.  **Regularization:** This is where XGBoost really shines. It incorporates regularization techniques to prevent **overfitting** (when a model performs very well on training data but poorly on new, unseen data). XGBoost uses two types of regularization:\n",
       "    *   **L1 (Lasso) Regularization:** Encourages sparsity by shrinking coefficients of less important features to zero.\n",
       "    *   **L2 (Ridge) Regularization:** Shrinks coefficients towards zero, preventing them from becoming too large.\n",
       "    *   XGBoost also regularizes the **complexity of the trees** themselves, controlling the number of leaf nodes and the gain at each split.\n",
       "\n",
       "5.  **Key Optimizations:** XGBoost is known for its speed and efficiency due to several optimizations:\n",
       "    *   **Parallel Processing:** Can utilize multiple CPU cores to speed up training.\n",
       "    *   **Cache-Aware Access:** Optimizes how data is accessed from memory.\n",
       "    *   **Out-of-Core Computation:** Can handle datasets that are too large to fit into memory.\n",
       "    *   **Sparsity-Aware Split Finding:** Efficiently handles datasets with many missing values.\n",
       "    *   **Tree Pruning:** Prunes branches of the tree that don't add significant value, preventing overfitting.\n",
       "\n",
       "**In essence, XGBoost builds a strong predictive model by iteratively adding decision trees that learn from the mistakes of the previous trees, while also incorporating sophisticated regularization techniques to ensure generalization.**\n",
       "\n",
       "## Real-Life Use Cases of XGBoost\n",
       "\n",
       "XGBoost has become a go-to algorithm for many machine learning tasks due to its performance, speed, and robustness. Here are some prominent real-life use cases:\n",
       "\n",
       "### 1. Fraud Detection\n",
       "\n",
       "*   **How it works:** Financial institutions use XGBoost to identify fraudulent transactions. The algorithm learns patterns from historical transaction data, including amounts, locations, transaction types, and customer behavior. When a new transaction occurs, XGBoost predicts the probability of it being fraudulent.\n",
       "*   **Why XGBoost is good:** Its ability to handle complex, non-linear relationships in data, along with its robustness to imbalanced datasets (where fraudulent transactions are rare), makes it highly effective. Regularization helps prevent flagging legitimate transactions as fraudulent (false positives).\n",
       "*   **Example:** Detecting credit card fraud, insurance claim fraud, or online payment fraud.\n",
       "\n",
       "### 2. Click-Through Rate (CTR) Prediction\n",
       "\n",
       "*   **How it works:** Online advertising platforms use XGBoost to predict the likelihood of a user clicking on an ad. Features include user demographics, browsing history, ad content, and website context.\n",
       "*   **Why XGBoost is good:** It can capture subtle interactions between features that influence click behavior. The speed of XGBoost is crucial for real-time ad serving decisions.\n",
       "*   **Example:** Google Ads, Facebook Ads, and other platforms optimizing ad placement and targeting.\n",
       "\n",
       "### 3. Recommendation Systems\n",
       "\n",
       "*   **How it works:** XGBoost can be used to build personalized recommendation engines. For example, in e-commerce, it can predict which products a user is most likely to buy based on their past purchases, browsing history, and the behavior of similar users.\n",
       "*   **Why XGBoost is good:** It can learn complex user preferences and item characteristics, leading to more accurate and relevant recommendations.\n",
       "*   **Example:** Recommending movies on Netflix, products on Amazon, or articles on news websites.\n",
       "\n",
       "### 4. Predictive Maintenance\n",
       "\n",
       "*   **How it works:** In industries like manufacturing and aviation, XGBoost can predict when machinery is likely to fail. It analyzes sensor data (temperature, vibration, pressure), operational history, and maintenance logs to identify early signs of potential issues.\n",
       "*   **Why XGBoost is good:** Its ability to handle time-series data and identify subtle anomalies makes it suitable for predicting failures before they happen, allowing for proactive maintenance and reducing downtime.\n",
       "*   **Example:** Predicting when a factory machine needs servicing, when an aircraft engine might fail, or when a wind turbine needs maintenance.\n",
       "\n",
       "### 5. Credit Scoring and Risk Assessment\n",
       "\n",
       "*   **How it works:** Banks and lending institutions use XGBoost to assess the creditworthiness of loan applicants. It analyzes financial history, income, employment, and other relevant factors to predict the probability of default.\n",
       "*   **Why XGBoost is good:** Its accuracy in predicting risk helps institutions make better lending decisions and manage their portfolios. The interpretability of feature importance can also be valuable for regulatory compliance.\n",
       "*   **Example:** Determining loan eligibility, setting interest rates, and assessing investment risk.\n",
       "\n",
       "### 6. Medical Diagnosis and Prognosis\n",
       "\n",
       "*   **How it works:** In healthcare, XGBoost can be used for tasks like diagnosing diseases based on patient symptoms, medical images, and genetic data, or predicting patient outcomes and treatment response.\n",
       "*   **Why XGBoost is good:** It can identify complex relationships between various health indicators and disease states, leading to more accurate diagnoses and personalized treatment plans.\n",
       "*   **Example:** Predicting the likelihood of a patient developing a certain disease, classifying tumor types from medical images, or predicting patient survival rates.\n",
       "\n",
       "### 7. Customer Churn Prediction\n",
       "\n",
       "*   **How it works:** Businesses use XGBoost to identify customers who are likely to stop using their services. Features include customer demographics, usage patterns, customer service interactions, and subscription details.\n",
       "*   **Why XGBoost is good:** By identifying at-risk customers, businesses can implement targeted retention strategies to prevent churn and maintain revenue.\n",
       "*   **Example:** Telecom companies predicting which customers are likely to switch providers, or subscription-based services identifying users who might cancel their subscriptions.\n",
       "\n",
       "### 8. Natural Language Processing (NLP) Tasks\n",
       "\n",
       "*   **How it works:** While deep learning models often dominate NLP, XGBoost can be effective for certain tasks, especially when dealing with structured or semi-structured text data. It can be used for sentiment analysis, text classification, and topic modeling by engineering relevant features from text.\n",
       "*   **Why XGBoost is good:** It offers a good balance of performance and interpretability for specific NLP problems, especially when computational resources are a concern or when dealing with feature engineering.\n",
       "*   **Example:** Classifying customer reviews as positive or negative, categorizing news articles, or identifying spam emails.\n",
       "\n",
       "### 9. Kaggle Competitions\n",
       "\n",
       "*   **Why XGBoost is good:** XGBoost is a perennial winner in many machine learning competitions on platforms like Kaggle. Its performance and flexibility make it a top choice for data scientists aiming for high accuracy on diverse datasets.\n",
       "\n",
       "**In summary, XGBoost is a powerful and versatile algorithm that has revolutionized many areas of data science and machine learning. Its ability to handle complex data, prevent overfitting, and deliver high accuracy makes it a valuable tool for solving a wide range of real-world problems.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top-k value 4, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## XGBoost: Extreme Gradient Boosting Explained with Real-Life Use Cases\n",
       "\n",
       "XGBoost, short for **Extreme Gradient Boosting**, is a powerful and highly efficient open-source implementation of the **gradient boosting algorithm**. It's become a go-to choice for data scientists and machine learning engineers due to its speed, performance, and flexibility.\n",
       "\n",
       "Let's break down the core concepts and then dive into real-life use cases.\n",
       "\n",
       "### Understanding the Core Concept: Gradient Boosting\n",
       "\n",
       "Before XGBoost, it's crucial to understand its foundation: **Gradient Boosting**.\n",
       "\n",
       "Imagine you're trying to predict a house price.\n",
       "\n",
       "1.  **Initial Guess:** You start with a simple, often naive, prediction (e.g., the average house price in the area). This prediction will likely be inaccurate.\n",
       "\n",
       "2.  **Identify Errors (Residuals):** You calculate the difference between your prediction and the actual house price. These differences are called **residuals** or **errors**.\n",
       "\n",
       "3.  **Build a Weak Learner to Correct Errors:** You train a **weak learner** (typically a decision tree) to predict these residuals. This weak learner focuses on the mistakes made by the previous prediction.\n",
       "\n",
       "4.  **Add the Weak Learner's Prediction:** You add the prediction from the weak learner to your initial guess. This new prediction is hopefully more accurate.\n",
       "\n",
       "5.  **Repeat:** You repeat steps 2-4. Each new weak learner focuses on the *remaining* errors, gradually improving the overall prediction.\n",
       "\n",
       "**Think of it like a team of experts:** The first expert makes a guess. The second expert identifies the flaws in the first expert's guess and tries to correct them. The third expert identifies the flaws in the *combined* guess of the first two, and so on. Each subsequent expert builds upon the work of the previous ones, leading to a more refined and accurate final prediction.\n",
       "\n",
       "### What Makes XGBoost \"Extreme\"?\n",
       "\n",
       "XGBoost takes the gradient boosting concept and supercharges it with several key optimizations and features:\n",
       "\n",
       "*   **Regularization:** XGBoost includes L1 and L2 regularization techniques, which help prevent overfitting. This means it's less likely to memorize the training data and generalize better to new, unseen data.\n",
       "*   **Parallel Processing:** It can leverage multiple CPU cores to train trees in parallel, significantly speeding up the training process.\n",
       "*   **Handling Missing Values:** XGBoost has a built-in mechanism to handle missing values in your dataset, which is a common problem in real-world data.\n",
       "*   **Tree Pruning:** It employs advanced pruning techniques to control tree complexity, further mitigating overfitting.\n",
       "*   **Cache Awareness:** It's designed to efficiently utilize hardware caches, leading to faster computations.\n",
       "*   **Flexibility:** It can be used for various tasks, including regression, classification, and ranking.\n",
       "\n",
       "### Key Components and How They Work Together:\n",
       "\n",
       "*   **Weak Learners (Decision Trees):** XGBoost, like other gradient boosting algorithms, primarily uses **decision trees** as its weak learners. These trees are typically \"shallow\" or have limited depth, making them \"weak\" individually but powerful when combined.\n",
       "*   **Gradient Descent:** The \"gradient\" in gradient boosting refers to using **gradient descent** to minimize the loss function. In each iteration, the algorithm calculates the gradient of the loss function with respect to the current predictions and uses this information to guide the next weak learner.\n",
       "*   **Loss Function:** This is a mathematical function that quantifies the error of the model. XGBoost can use various loss functions depending on the problem (e.g., Mean Squared Error for regression, Log Loss for classification).\n",
       "*   **Objective Function:** This function combines the loss function with regularization terms, aiming to minimize both prediction errors and model complexity.\n",
       "\n",
       "### Real-Life Use Cases of XGBoost:\n",
       "\n",
       "XGBoost's power and efficiency have made it a popular choice across a wide range of industries. Here are some prominent real-life use cases:\n",
       "\n",
       "**1. E-commerce and Retail:**\n",
       "\n",
       "*   **Product Recommendation Systems:** XGBoost can predict which products a user is most likely to buy based on their past purchase history, browsing behavior, and demographic information.\n",
       "    *   *Example:* Amazon recommending products based on your browsing history and what similar users have purchased.\n",
       "*   **Customer Churn Prediction:** Identifying customers who are likely to stop using a service or product. This allows businesses to proactively engage with at-risk customers and offer incentives to retain them.\n",
       "    *   *Example:* A telecom company predicting which customers are likely to switch to a competitor.\n",
       "*   **Sales Forecasting:** Predicting future sales volumes for products or stores, enabling better inventory management and resource allocation.\n",
       "    *   *Example:* A grocery store chain predicting demand for specific produce items to optimize stocking.\n",
       "*   **Fraud Detection:** Identifying fraudulent transactions in real-time by analyzing transaction patterns, user behavior, and other relevant data.\n",
       "    *   *Example:* Credit card companies detecting suspicious transactions that deviate from a user's typical spending habits.\n",
       "\n",
       "**2. Finance and Banking:**\n",
       "\n",
       "*   **Credit Risk Assessment:** Evaluating the likelihood of a borrower defaulting on a loan by analyzing their financial history, credit score, and other relevant factors.\n",
       "    *   *Example:* Banks deciding whether to approve a loan application and at what interest rate.\n",
       "*   **Algorithmic Trading:** Building models that predict stock price movements or other financial market indicators to execute trades automatically.\n",
       "    *   *Example:* Hedge funds using XGBoost to identify trading opportunities in the stock market.\n",
       "*   **Loan Default Prediction:** Similar to credit risk, but specifically focusing on predicting defaults in existing loan portfolios.\n",
       "*   **Fraudulent Insurance Claims:** Detecting suspicious insurance claims that might be fraudulent by analyzing claim details, historical data, and patterns.\n",
       "    *   *Example:* An insurance company flagging a claim that has unusual characteristics compared to similar claims.\n",
       "\n",
       "**3. Healthcare:**\n",
       "\n",
       "*   **Disease Prediction and Diagnosis:** Building models to predict the likelihood of a patient developing a specific disease or to aid in the diagnosis of existing conditions based on patient data, medical history, and test results.\n",
       "    *   *Example:* Predicting a patient's risk of developing diabetes based on their lifestyle, genetics, and medical history.\n",
       "*   **Drug Discovery and Development:** Identifying potential drug candidates by analyzing molecular structures and predicting their efficacy and safety.\n",
       "*   **Patient Readmission Prediction:** Identifying patients who are at high risk of being readmitted to the hospital shortly after discharge, allowing for better post-discharge care.\n",
       "    *   *Example:* Hospitals identifying patients who need extra support to prevent them from returning to the hospital.\n",
       "\n",
       "**4. Technology and Web Services:**\n",
       "\n",
       "*   **Search Engine Ranking:** Optimizing search results by predicting the relevance of web pages to user queries.\n",
       "    *   *Example:* Google's search algorithm uses machine learning models, including boosted trees, to rank search results.\n",
       "*   **Spam Filtering:** Identifying and filtering out unwanted emails or messages by analyzing their content and sender characteristics.\n",
       "    *   *Example:* Gmail's spam filter uses complex models to identify and quarantine spam emails.\n",
       "*   **Ad Click-Through Rate (CTR) Prediction:** Predicting the likelihood of a user clicking on an advertisement, which is crucial for targeted advertising.\n",
       "    *   *Example:* Online advertising platforms predicting which ads will be most engaging for specific users.\n",
       "\n",
       "**5. Other Industries:**\n",
       "\n",
       "*   **Manufacturing:** Predicting equipment failures to enable proactive maintenance and reduce downtime.\n",
       "    *   *Example:* Factories predicting when a machine is likely to break down to schedule maintenance before it happens.\n",
       "*   **Environmental Science:** Predicting weather patterns, air quality, or the impact of climate change.\n",
       "*   **Human Resources:** Predicting employee turnover or identifying candidates most likely to succeed in a role.\n",
       "\n",
       "### Why is XGBoost So Popular?\n",
       "\n",
       "*   **High Accuracy:** It consistently achieves state-of-the-art performance on many machine learning tasks.\n",
       "*   **Speed and Efficiency:** Its optimized implementation makes it faster than many other gradient boosting libraries.\n",
       "*   **Scalability:** It can handle large datasets and complex problems.\n",
       "*   **Robustness:** It's effective in dealing with noisy data and missing values.\n",
       "*   **Versatility:** It can be applied to a wide range of problems with different data types.\n",
       "*   **Strong Community Support:** It has a large and active community, meaning plenty of resources, tutorials, and support are available.\n",
       "\n",
       "In essence, XGBoost provides a powerful and efficient engine for building predictive models that can tackle complex, real-world challenges with impressive accuracy. Its ability to learn from mistakes iteratively and incorporate regularization makes it a robust and reliable tool in the data scientist's arsenal."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top-k value 16, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's dive into the world of XGBoost!\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "XGBoost stands for **eXtreme Gradient Boosting**. It's a powerful and highly efficient open-source implementation of the gradient boosting algorithm. In simpler terms, it's a machine learning technique that builds a strong predictive model by combining many weak predictive models (typically decision trees) in a sequential and additive way.\n",
       "\n",
       "Here's a breakdown of the core concepts:\n",
       "\n",
       "1.  **Ensemble Learning:** XGBoost is an ensemble method. This means it combines multiple individual models to create a more robust and accurate overall model. Think of it like a group of experts collaborating on a problem, each offering their perspective.\n",
       "\n",
       "2.  **Gradient Boosting:**\n",
       "    *   **Boosting:** Unlike \"bagging\" methods (like Random Forests) which build models independently, boosting methods build models sequentially. Each new model tries to correct the errors made by the previous ones.\n",
       "    *   **Gradient:** The \"gradient\" part refers to the use of gradient descent optimization. In each step, XGBoost calculates the gradient of the loss function (which measures how \"wrong\" the model's predictions are) with respect to the predictions. It then builds a new tree to predict these \"residuals\" or errors, effectively reducing the overall error.\n",
       "\n",
       "3.  **Key Features of XGBoost (What makes it \"eXtreme\"):**\n",
       "\n",
       "    *   **Regularization:** XGBoost incorporates L1 (Lasso) and L2 (Ridge) regularization into its objective function. This helps prevent overfitting by penalizing complex models, leading to better generalization on unseen data.\n",
       "    *   **Tree Pruning:** It uses sophisticated tree pruning techniques to control the complexity of individual decision trees, further mitigating overfitting.\n",
       "    *   **Handling Missing Values:** XGBoost has a built-in mechanism to handle missing values in the data, making it more robust to real-world datasets.\n",
       "    *   **Parallel Processing:** It can leverage multiple CPU cores to speed up training, making it significantly faster than many other gradient boosting implementations.\n",
       "    *   **Cache-Aware Access:** XGBoost is designed to efficiently utilize CPU cache, further optimizing performance.\n",
       "    *   **Out-of-Core Computing:** It can handle datasets that are too large to fit into memory by processing them in chunks.\n",
       "    *   **Early Stopping:** It monitors performance on a validation set during training and stops when performance starts to degrade, preventing overfitting.\n",
       "    *   **Weighted Quantile Sketch:** An efficient algorithm for finding approximate quantiles, which helps in splitting trees.\n",
       "\n",
       "**In essence, XGBoost is a highly optimized, feature-rich, and performant gradient boosting algorithm that has become a go-to choice for many data scientists and machine learning practitioners.**\n",
       "\n",
       "## Real-Life Use Cases of XGBoost\n",
       "\n",
       "XGBoost's versatility, accuracy, and speed have made it a dominant force in various industries and applications. Here are some prominent real-life use cases:\n",
       "\n",
       "**1. E-commerce and Retail:**\n",
       "\n",
       "*   **Customer Churn Prediction:** Identifying customers who are likely to stop using a service or purchasing products. XGBoost can analyze customer behavior, demographics, and past interactions to predict churn with high accuracy.\n",
       "    *   *Example:* A streaming service uses XGBoost to predict which subscribers are likely to cancel their subscriptions based on their viewing habits, engagement, and customer service interactions.\n",
       "*   **Recommendation Systems:** Suggesting products or content to users based on their past behavior and similarities with other users. XGBoost can be used to rank potential recommendations.\n",
       "    *   *Example:* An online retailer uses XGBoost to predict which products a customer is most likely to buy next, based on their browsing history, purchase history, and demographic information.\n",
       "*   **Fraud Detection:** Identifying fraudulent transactions or activities. XGBoost can learn patterns associated with fraudulent behavior.\n",
       "    *   *Example:* A credit card company uses XGBoost to detect fraudulent credit card transactions by analyzing transaction amounts, locations, merchant types, and other features.\n",
       "*   **Demand Forecasting:** Predicting future demand for products or services.\n",
       "    *   *Example:* A supermarket chain uses XGBoost to predict the demand for perishable goods to optimize inventory management and reduce waste.\n",
       "\n",
       "**2. Finance:**\n",
       "\n",
       "*   **Credit Risk Assessment:** Evaluating the likelihood of a borrower defaulting on a loan. XGBoost can analyze financial history, credit scores, and other factors to assess risk.\n",
       "    *   *Example:* A bank uses XGBoost to predict the probability of a loan applicant defaulting on a mortgage, helping them make informed lending decisions.\n",
       "*   **Algorithmic Trading:** Developing trading strategies by predicting stock price movements or market trends.\n",
       "    *   *Example:* A hedge fund uses XGBoost to predict short-term stock price movements, informing their high-frequency trading strategies.\n",
       "*   **Loan Default Prediction:** Similar to credit risk, predicting when a borrower will miss loan payments.\n",
       "\n",
       "**3. Healthcare:**\n",
       "\n",
       "*   **Disease Prediction and Diagnosis:** Identifying patients at risk of developing certain diseases or predicting the likelihood of a specific diagnosis.\n",
       "    *   *Example:* A research hospital uses XGBoost to predict the risk of developing diabetes in patients based on their medical history, lifestyle factors, and genetic predispositions.\n",
       "*   **Drug Discovery and Development:** Predicting the efficacy and potential side effects of new drugs.\n",
       "*   **Patient Readmission Prediction:** Identifying patients who are likely to be readmitted to the hospital shortly after discharge.\n",
       "\n",
       "**4. Telecommunications:**\n",
       "\n",
       "*   **Customer Churn Prediction:** (Similar to e-commerce) Identifying customers likely to switch to a competitor.\n",
       "    *   *Example:* A mobile carrier uses XGBoost to predict which customers are likely to switch to a competitor based on their call patterns, data usage, contract status, and customer service interactions.\n",
       "*   **Network Anomaly Detection:** Identifying unusual patterns in network traffic that could indicate a security breach or operational issue.\n",
       "\n",
       "**5. Manufacturing:**\n",
       "\n",
       "*   **Predictive Maintenance:** Predicting when machinery is likely to fail, allowing for proactive maintenance and preventing costly downtime.\n",
       "    *   *Example:* A manufacturing plant uses XGBoost to analyze sensor data from its production machinery to predict when a component is likely to fail, scheduling maintenance before a breakdown occurs.\n",
       "*   **Quality Control:** Identifying defects in manufactured products.\n",
       "\n",
       "**6. Natural Language Processing (NLP):**\n",
       "\n",
       "*   **Text Classification:** Categorizing text documents into predefined classes (e.g., spam detection, sentiment analysis). XGBoost can be used with feature-engineered text data.\n",
       "    *   *Example:* A company uses XGBoost to classify customer feedback as positive, negative, or neutral to understand customer sentiment.\n",
       "*   **Spam Detection:** Filtering out unwanted emails.\n",
       "\n",
       "**7. Kaggle Competitions and Data Science Challenges:**\n",
       "\n",
       "XGBoost has been a consistent winner in many Kaggle competitions, demonstrating its superior performance on tabular data across a wide range of problems. This is a testament to its power and effectiveness.\n",
       "\n",
       "**Why is XGBoost so popular for these use cases?**\n",
       "\n",
       "*   **Accuracy:** It consistently achieves state-of-the-art results on many tasks.\n",
       "*   **Speed:** Its optimized implementation makes it suitable for large datasets.\n",
       "*   **Flexibility:** It can handle various data types and is easily tunable.\n",
       "*   **Robustness:** Its regularization and handling of missing values make it reliable.\n",
       "\n",
       "In summary, XGBoost is a powerful tool that has revolutionized how we approach many predictive modeling tasks. Its ability to deliver accurate and efficient results makes it a cornerstone of modern data science."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top-k value 32, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's dive into the concept of XGBoost and explore its real-life applications.\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "XGBoost stands for **eXtreme Gradient Boosting**. It's a powerful, open-source, and highly efficient machine learning algorithm based on the **gradient boosting** framework. It has gained immense popularity in the data science and machine learning community due to its speed, accuracy, and flexibility.\n",
       "\n",
       "At its core, XGBoost is an **ensemble learning method**. This means it combines multiple \"weak learners\" (typically decision trees) to create a single, strong predictive model. The \"boosting\" aspect refers to an iterative process where each new tree is trained to correct the errors made by the previous trees.\n",
       "\n",
       "Here's a breakdown of its key characteristics:\n",
       "\n",
       "**1. Gradient Boosting Foundation:**\n",
       "\n",
       "*   **Weak Learners:** XGBoost uses decision trees as its base learners. These are often \"shallow\" trees, meaning they don't grow too deep, which helps prevent overfitting.\n",
       "*   **Sequential Learning:** Unlike methods like Random Forests where trees are built independently, in gradient boosting, trees are built sequentially. Each new tree focuses on predicting the *residuals* (the errors) of the combined predictions from the previous trees.\n",
       "*   **Gradient Descent Analogy:** The \"gradient\" in gradient boosting refers to the use of gradient descent optimization techniques to minimize the loss function (the measure of error) by iteratively adjusting the model.\n",
       "\n",
       "**2. eXtreme Enhancements:**\n",
       "\n",
       "XGBoost takes the standard gradient boosting algorithm and introduces several key optimizations that make it \"extreme\":\n",
       "\n",
       "*   **Regularization (L1 & L2):** XGBoost incorporates regularization terms directly into its objective function. This helps to prevent overfitting by penalizing complex models, leading to better generalization on unseen data.\n",
       "*   **Tree Pruning:** It uses a \"depth-first\" approach with a **maximum depth** parameter, but also allows for pruning trees even further if certain conditions are met (e.g., if a split doesn't improve the objective function). This further combats overfitting.\n",
       "*   **Handling Missing Values:** XGBoost has a built-in mechanism to handle missing values in the data. Instead of imputing them, it learns the best direction to send samples with missing values during tree traversal.\n",
       "*   **Parallel Processing:** While boosting is inherently sequential, XGBoost can parallelize the *tree construction* process (e.g., finding the best split for each node) across multiple CPU cores, significantly speeding up training.\n",
       "*   **Cache-Aware Access:** It's designed to be cache-efficient, meaning it optimizes data access to the CPU cache for faster computations.\n",
       "*   **Out-of-Core Computation:** For datasets that don't fit into memory, XGBoost can handle them by processing data in chunks, allowing it to work with extremely large datasets.\n",
       "*   **Weighted Quantile Sketch:** This advanced technique allows for efficient calculation of approximate quantiles, which is crucial for finding optimal split points in the trees, especially on large datasets.\n",
       "\n",
       "**In essence, XGBoost is a highly optimized and robust gradient boosting algorithm that excels at:**\n",
       "\n",
       "*   **Accuracy:** Often achieves state-of-the-art performance on various tabular datasets.\n",
       "*   **Speed:** Significantly faster than traditional gradient boosting implementations.\n",
       "*   **Flexibility:** Can be used for both classification and regression tasks.\n",
       "*   **Robustness:** Handles missing values and prevents overfitting effectively.\n",
       "\n",
       "## Real-Life Use Cases of XGBoost\n",
       "\n",
       "XGBoost's power and versatility have made it a go-to algorithm for a wide range of real-world problems across various industries. Here are some prominent examples:\n",
       "\n",
       "**1. Finance:**\n",
       "\n",
       "*   **Credit Risk Assessment:** Predicting the likelihood of a customer defaulting on a loan or credit card. XGBoost can analyze a multitude of customer features (income, credit history, debt-to-income ratio, etc.) to identify high-risk individuals.\n",
       "    *   **Example:** A bank using XGBoost to score loan applications, automatically approving low-risk ones and flagging high-risk ones for further manual review.\n",
       "*   **Fraud Detection:** Identifying fraudulent transactions (e.g., credit card fraud, insurance claims fraud). XGBoost can learn complex patterns that differentiate fraudulent activities from legitimate ones.\n",
       "    *   **Example:** An e-commerce platform using XGBoost to flag suspicious transactions in real-time, preventing financial losses.\n",
       "*   **Algorithmic Trading:** Developing trading strategies by predicting stock price movements or market trends.\n",
       "    *   **Example:** A hedge fund using XGBoost to forecast the price of a particular stock, informing their buy/sell decisions.\n",
       "\n",
       "**2. E-commerce and Retail:**\n",
       "\n",
       "*   **Customer Churn Prediction:** Identifying customers who are likely to stop using a service or product. This allows businesses to proactively offer incentives or address issues to retain them.\n",
       "    *   **Example:** A subscription service using XGBoost to predict which customers are likely to cancel their subscriptions next month, enabling targeted retention campaigns.\n",
       "*   **Product Recommendation Systems:** While often powered by collaborative filtering or deep learning, XGBoost can be used to enhance recommendation systems by modeling user preferences based on various features.\n",
       "    *   **Example:** An online retailer using XGBoost to predict which products a user is most likely to purchase based on their browsing history, past purchases, and demographic information.\n",
       "*   **Sales Forecasting:** Predicting future sales volume for products or stores. This helps with inventory management, resource allocation, and marketing planning.\n",
       "    *   **Example:** A supermarket chain using XGBoost to forecast the demand for specific products in different stores, optimizing stock levels and reducing waste.\n",
       "\n",
       "**3. Healthcare:**\n",
       "\n",
       "*   **Disease Prediction and Diagnosis:** Predicting the likelihood of a patient developing a certain disease or assisting in diagnosis based on patient data (medical history, symptoms, lab results).\n",
       "    *   **Example:** A hospital using XGBoost to predict the risk of readmission for patients with chronic diseases, allowing for proactive interventions.\n",
       "*   **Drug Discovery and Development:** Identifying potential drug candidates or predicting the efficacy of new treatments.\n",
       "    *   **Example:** A pharmaceutical company using XGBoost to analyze molecular data and predict which compounds are most likely to be effective against a particular disease.\n",
       "*   **Patient Risk Stratification:** Identifying patients who are at higher risk of adverse outcomes, allowing for personalized care plans.\n",
       "    *   **Example:** An insurance company using XGBoost to identify high-cost patients, enabling them to offer preventative care programs.\n",
       "\n",
       "**4. Marketing:**\n",
       "\n",
       "*   **Customer Lifetime Value (CLV) Prediction:** Estimating the total revenue a customer is expected to generate over their relationship with a business.\n",
       "    *   **Example:** A SaaS company using XGBoost to predict the CLV of new customers, allowing them to optimize customer acquisition strategies.\n",
       "*   **Marketing Campaign Optimization:** Predicting which marketing channels or campaigns will be most effective for reaching target audiences.\n",
       "    *   **Example:** A marketing team using XGBoost to determine the best channels to reach specific customer segments for a new product launch.\n",
       "\n",
       "**5. Technology and Engineering:**\n",
       "\n",
       "*   **Anomaly Detection:** Identifying unusual or outlier events in data, which can be useful for cybersecurity, system monitoring, or industrial fault detection.\n",
       "    *   **Example:** A cybersecurity firm using XGBoost to detect malicious network activity by identifying patterns that deviate from normal behavior.\n",
       "*   **Predictive Maintenance:** Predicting when equipment is likely to fail, allowing for proactive maintenance and preventing costly downtime.\n",
       "    *   **Example:** A manufacturing plant using XGBoost to monitor sensor data from machinery and predict when a component is likely to fail, scheduling maintenance before an actual breakdown.\n",
       "\n",
       "**6. Social Media and Natural Language Processing (NLP):**\n",
       "\n",
       "*   **Sentiment Analysis:** Determining the emotional tone (positive, negative, neutral) of text data, such as customer reviews or social media posts.\n",
       "    *   **Example:** A company using XGBoost to analyze customer feedback on social media to gauge public perception of their brand.\n",
       "*   **Spam Detection:** Identifying and filtering out spam emails or messages.\n",
       "\n",
       "**Key Advantages that Drive these Use Cases:**\n",
       "\n",
       "*   **High Predictive Accuracy:** XGBoost often outperforms other algorithms, especially on structured (tabular) data.\n",
       "*   **Speed and Efficiency:** Its optimized architecture allows for faster training and prediction, crucial for real-time applications.\n",
       "*   **Robustness to Overfitting:** Regularization techniques and tree pruning make it a reliable choice even with complex datasets.\n",
       "*   **Feature Importance:** XGBoost provides insights into which features are most important for making predictions, aiding in understanding and interpretability.\n",
       "\n",
       "In summary, XGBoost has become a powerful tool in the data scientist's arsenal due to its combination of accuracy, speed, and robustness, making it a valuable solution for a vast array of real-world prediction and classification problems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top-k value 40, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## XGBoost: The Supercharged Decision Tree Ensemble\n",
       "\n",
       "XGBoost, short for **Extreme Gradient Boosting**, is a powerful and highly efficient machine learning algorithm that has become a favorite for its speed, performance, and accuracy. At its core, it's an implementation of **gradient boosting**, a technique that builds predictive models by sequentially adding decision trees. However, XGBoost takes this concept to the \"extreme\" by incorporating several optimizations and regularization techniques.\n",
       "\n",
       "Let's break down the core concepts:\n",
       "\n",
       "**1. Decision Trees (The Building Blocks):**\n",
       "\n",
       "Imagine a series of \"if-then-else\" questions designed to arrive at a prediction. That's essentially a decision tree. For example, if you're predicting house prices:\n",
       "\n",
       "*   Is the house larger than 2000 sq ft? (Yes/No)\n",
       "*   If Yes, is it in a good school district? (Yes/No)\n",
       "*   ...and so on.\n",
       "\n",
       "Each question splits the data into subgroups, and the final \"leaf\" nodes provide the prediction.\n",
       "\n",
       "**2. Ensemble Learning (Strength in Numbers):**\n",
       "\n",
       "While a single decision tree can be effective, it can also be prone to overfitting (learning the training data too well and failing on new data). Ensemble methods combine multiple \"weak\" learners (like decision trees) to create a \"strong\" learner.\n",
       "\n",
       "**3. Gradient Boosting (Learning from Mistakes):**\n",
       "\n",
       "This is where the magic of boosting comes in. Gradient boosting builds trees sequentially, with each new tree trying to **correct the errors** made by the previous ones.\n",
       "\n",
       "*   **Tree 1:** Makes an initial prediction.\n",
       "*   **Tree 2:** Focuses on the data points where Tree 1 made the biggest errors (the residuals). It tries to predict these errors.\n",
       "*   **Tree 3:** Focuses on the remaining errors after Tree 2 has made its corrections.\n",
       "*   ...and so on.\n",
       "\n",
       "Each subsequent tree adds a \"correction\" to the overall prediction, gradually improving accuracy.\n",
       "\n",
       "**4. The \"Extreme\" in XGBoost (The Innovations):**\n",
       "\n",
       "XGBoost enhances gradient boosting with several key features:\n",
       "\n",
       "*   **Regularization:** Prevents overfitting by penalizing complex trees. This includes:\n",
       "    *   **L1 (Lasso) and L2 (Ridge) regularization on leaf weights:** Encourages simpler models.\n",
       "    *   **Tree pruning:** Stops tree growth early if it doesn't contribute significantly to the prediction.\n",
       "*   **Parallel Processing:** Can utilize multiple CPU cores to build trees much faster.\n",
       "*   **Handling Missing Values:** Intelligently learns the best way to impute missing data within the tree-building process.\n",
       "*   **Cache-Aware Access:** Optimizes memory usage for faster computation.\n",
       "*   **Tree Pruning (More Advanced):** Instead of just stopping growth, XGBoost uses a \"max depth\" parameter and then prunes back trees if the gain from splitting is not significant.\n",
       "*   **Split Finding Optimization:** Uses an algorithm to find the best split points more efficiently.\n",
       "*   **Cross-Validation:** Built-in cross-validation allows for robust model evaluation.\n",
       "\n",
       "**How it Works (Simplified):**\n",
       "\n",
       "Imagine you're trying to predict a student's exam score.\n",
       "\n",
       "1.  **Initial Guess:** You make a rough prediction for all students (e.g., everyone gets a 70).\n",
       "2.  **First Tree (Error Correction):** You build a decision tree that looks at factors like study hours and previous grades to predict the *difference* between the actual score and your initial guess. This tree might identify that students who study more tend to score higher.\n",
       "3.  **Second Tree (Further Correction):** You build another tree that focuses on the remaining errors from the first tree. This tree might identify that students who also attended review sessions tend to do even better.\n",
       "4.  **Iterative Improvement:** You continue this process, with each new tree learning from the mistakes of the previous ones. The final prediction is the sum of the initial guess and the predictions from all the trees.\n",
       "\n",
       "XGBoost does this in a highly optimized and regularized way, leading to its impressive performance.\n",
       "\n",
       "---\n",
       "\n",
       "## Real-Life Use Cases of XGBoost:\n",
       "\n",
       "XGBoost is incredibly versatile and has found success in a wide range of applications across various industries. Here are some prominent examples:\n",
       "\n",
       "**1. Finance:**\n",
       "\n",
       "*   **Credit Risk Assessment:** Predicting the likelihood of a borrower defaulting on a loan. XGBoost can analyze a vast number of customer features to identify patterns associated with default.\n",
       "*   **Fraud Detection:** Identifying fraudulent transactions (e.g., credit card fraud, insurance claims). By analyzing transaction patterns, customer behavior, and other data, XGBoost can flag suspicious activities.\n",
       "*   **Algorithmic Trading:** Developing strategies for automated stock trading by predicting price movements based on historical data, news sentiment, and other market indicators.\n",
       "*   **Loan Underwriting:** Automating the process of approving or rejecting loan applications by assessing risk and eligibility.\n",
       "\n",
       "**2. E-commerce and Retail:**\n",
       "\n",
       "*   **Recommendation Systems:** Suggesting products to customers based on their past purchases, browsing history, and the behavior of similar users.\n",
       "*   **Customer Churn Prediction:** Identifying customers who are likely to stop using a service or buying products. This allows businesses to proactively engage with these customers to retain them.\n",
       "*   **Sales Forecasting:** Predicting future sales volumes to optimize inventory management, staffing, and marketing campaigns.\n",
       "*   **Price Optimization:** Dynamically adjusting prices of products based on demand, competitor pricing, and inventory levels.\n",
       "\n",
       "**3. Healthcare:**\n",
       "\n",
       "*   **Disease Prediction and Diagnosis:** Identifying patients at risk of developing certain diseases (e.g., diabetes, heart disease) or assisting in the diagnosis of existing conditions based on medical records, lab results, and symptoms.\n",
       "*   **Drug Discovery:** Predicting the efficacy and potential side effects of new drug compounds.\n",
       "*   **Patient Readmission Prediction:** Identifying patients who are likely to be readmitted to the hospital after discharge, allowing for better post-discharge care.\n",
       "*   **Medical Image Analysis:** While often used with deep learning, XGBoost can be used in conjunction with feature extraction from medical images for diagnostic tasks.\n",
       "\n",
       "**4. Marketing:**\n",
       "\n",
       "*   **Customer Segmentation:** Grouping customers into different segments based on their demographics, purchasing behavior, and preferences for targeted marketing campaigns.\n",
       "*   **Click-Through Rate (CTR) Prediction:** Predicting the likelihood of a user clicking on an online advertisement.\n",
       "*   **Campaign Performance Optimization:** Analyzing the effectiveness of marketing campaigns and identifying which strategies yield the best results.\n",
       "\n",
       "**5. Other Industries:**\n",
       "\n",
       "*   **Natural Language Processing (NLP):** Sentiment analysis, topic modeling, and text classification.\n",
       "*   **Sports Analytics:** Predicting game outcomes, player performance, and identifying promising talent.\n",
       "*   **Manufacturing:** Predictive maintenance of machinery to prevent breakdowns.\n",
       "*   **Human Resources:** Predicting employee performance and identifying candidates who are a good fit for a company culture.\n",
       "\n",
       "**Why XGBoost is So Popular in These Use Cases:**\n",
       "\n",
       "*   **High Accuracy:** It consistently achieves state-of-the-art results on many tabular datasets.\n",
       "*   **Speed and Efficiency:** Its optimized implementation makes it faster than many other gradient boosting algorithms.\n",
       "*   **Robustness:** Its regularization techniques help prevent overfitting, making it generalize well to unseen data.\n",
       "*   **Flexibility:** It can handle various data types and is well-suited for both classification and regression tasks.\n",
       "*   **Scalability:** Can handle large datasets efficiently.\n",
       "\n",
       "In essence, XGBoost is a powerful and reliable tool that can extract complex patterns from data to make accurate predictions, making it invaluable across a wide spectrum of real-world applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experiment with 'top_k' parameter \n",
    "for k in [1, 4, 16, 32, 40]: \n",
    "    config = genai.types.GenerationConfig(top_k=k) \n",
    "    result = get_response(\"Explain the concept of XGBoost with real-life use cases.\", generation_config=config) \n",
    "\n",
    "    print(f\"\\n\\nFor top-k value {k}, result: \\n\\n\") \n",
    "    display(Markdown(result.text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "611e1651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top-p value 0, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's break down XGBoost, a powerful and widely used machine learning algorithm, and then explore its real-life applications.\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "XGBoost stands for **eXtreme Gradient Boosting**. At its core, it's an implementation of the **gradient boosting** algorithm, but with significant optimizations and enhancements that make it incredibly fast and accurate.\n",
       "\n",
       "To understand XGBoost, let's first understand its building blocks:\n",
       "\n",
       "1.  **Ensemble Learning:** XGBoost is an ensemble method. This means it combines multiple \"weak\" learners (typically decision trees) to create a single, strong learner. The idea is that by combining many simple models, you can achieve better performance than any single model could on its own.\n",
       "\n",
       "2.  **Boosting:** Boosting is a specific type of ensemble learning where models are built sequentially. Each new model tries to correct the errors made by the previous models. It's like a team of students learning a subject: the first student learns, the second student learns from the first student's mistakes, and so on.\n",
       "\n",
       "3.  **Gradient Boosting:** This is where the \"gradient\" in XGBoost comes in. Gradient boosting works by minimizing a loss function (which measures how bad your model's predictions are). It does this by iteratively adding new models that are trained to predict the *residuals* (the errors) of the previous models. The \"gradient\" refers to the direction of steepest descent in the loss function, guiding the process of error correction.\n",
       "\n",
       "4.  **XGBoost's \"eXtreme\" Enhancements:** This is what sets XGBoost apart. It takes the core gradient boosting idea and makes it \"extreme\" through several key innovations:\n",
       "\n",
       "    *   **Regularization:** XGBoost incorporates L1 (Lasso) and L2 (Ridge) regularization directly into the objective function. This helps prevent overfitting by penalizing complex models, leading to better generalization on unseen data.\n",
       "    *   **Tree Pruning:** Unlike traditional gradient boosting that might grow trees to their full depth and then prune, XGBoost uses a \"depth-first\" approach and prunes trees based on a \"gain\" threshold. If splitting a node doesn't improve the objective function by a certain amount, it stops splitting.\n",
       "    *   **Handling Missing Values:** XGBoost has a built-in mechanism to handle missing values in the data. It learns the best direction to send instances with missing values during the tree splitting process.\n",
       "    *   **Parallel Processing:** XGBoost can leverage multiple CPU cores to speed up training. It can parallelize the process of finding the best split points for each tree.\n",
       "    *   **Cache-Aware Access:** It's designed to efficiently utilize CPU cache, further boosting performance.\n",
       "    *   **Out-of-Core Computation:** For datasets that don't fit into memory, XGBoost can handle them by processing data in chunks.\n",
       "    *   **Cross-Validation:** XGBoost has a built-in cross-validation function, making it easier to tune hyperparameters and assess model performance.\n",
       "\n",
       "**In essence, XGBoost is a highly optimized and regularized gradient boosting algorithm that is known for its speed, accuracy, and robustness.**\n",
       "\n",
       "## How it Works (Simplified Analogy)\n",
       "\n",
       "Imagine you're trying to predict the price of a house.\n",
       "\n",
       "1.  **Initial Guess:** You start with a very simple model, maybe just predicting the average price of all houses. This is your first \"weak learner.\"\n",
       "2.  **Identify Errors:** You look at the houses where your initial guess was wrong (the residuals). Some houses were predicted too low, others too high.\n",
       "3.  **Build a Correction Model:** You train a new, slightly more complex model (another decision tree) to predict these errors. For example, if your initial guess was too low for houses with many bedrooms, this new model will learn to add a \"bonus\" for more bedrooms.\n",
       "4.  **Combine and Refine:** You combine your initial guess with the predictions of the correction model. This gives you a better overall prediction.\n",
       "5.  **Repeat:** You repeat steps 2-4 many times. Each new model focuses on correcting the remaining errors of the combined previous models.\n",
       "6.  **XGBoost's Magic:** XGBoost does this very efficiently. It uses regularization to ensure the correction models don't become too complex and overfit. It also uses clever techniques to speed up the process of finding the best ways to correct errors.\n",
       "\n",
       "## Real-Life Use Cases of XGBoost\n",
       "\n",
       "XGBoost's power and versatility have made it a go-to algorithm for a wide range of problems across various industries. Here are some prominent real-life use cases:\n",
       "\n",
       "### 1. Finance\n",
       "\n",
       "*   **Credit Risk Assessment:** Predicting the likelihood of a borrower defaulting on a loan. XGBoost can analyze a vast number of features (income, credit history, loan amount, etc.) to build highly accurate risk models.\n",
       "    *   **Example:** Banks use XGBoost to decide whether to approve loan applications and at what interest rate.\n",
       "*   **Fraud Detection:** Identifying fraudulent transactions in credit card usage, insurance claims, or online activities. XGBoost can detect subtle patterns that might indicate fraudulent behavior.\n",
       "    *   **Example:** E-commerce platforms use XGBoost to flag suspicious transactions in real-time.\n",
       "*   **Algorithmic Trading:** Developing strategies for buying and selling financial assets based on market data. XGBoost can predict stock price movements or identify trading opportunities.\n",
       "    *   **Example:** Hedge funds use XGBoost to build predictive models for trading strategies.\n",
       "*   **Customer Churn Prediction:** Identifying customers who are likely to stop using a service. This allows companies to proactively offer incentives or address issues to retain them.\n",
       "    *   **Example:** Telecom companies use XGBoost to predict which customers are likely to switch providers.\n",
       "\n",
       "### 2. E-commerce and Retail\n",
       "\n",
       "*   **Recommendation Systems:** Suggesting products or content to users based on their past behavior and preferences. XGBoost can power sophisticated recommendation engines.\n",
       "    *   **Example:** Online retailers like Amazon use XGBoost to recommend products you might like.\n",
       "*   **Sales Forecasting:** Predicting future sales volumes for products or stores. This helps with inventory management, staffing, and marketing.\n",
       "    *   **Example:** Retailers use XGBoost to forecast demand for seasonal products.\n",
       "*   **Customer Lifetime Value (CLV) Prediction:** Estimating the total revenue a customer is expected to generate over their relationship with a company.\n",
       "    *   **Example:** Marketing teams use CLV predictions to allocate marketing budgets effectively.\n",
       "\n",
       "### 3. Healthcare\n",
       "\n",
       "*   **Disease Prediction and Diagnosis:** Identifying patients at risk of developing certain diseases or assisting in the diagnosis of medical conditions based on patient data.\n",
       "    *   **Example:** Researchers use XGBoost to predict the likelihood of a patient developing diabetes based on their medical history and lifestyle.\n",
       "*   **Drug Discovery:** Analyzing biological data to identify potential drug candidates or predict the efficacy of new treatments.\n",
       "    *   **Example:** Pharmaceutical companies use XGBoost to screen potential drug compounds.\n",
       "*   **Patient Readmission Prediction:** Identifying patients who are at high risk of being readmitted to the hospital shortly after discharge.\n",
       "    *   **Example:** Hospitals use XGBoost to identify patients who need extra post-discharge care.\n",
       "\n",
       "### 4. Technology and Internet\n",
       "\n",
       "*   **Search Engine Ranking:** Improving the relevance of search results by predicting which pages are most likely to satisfy a user's query.\n",
       "    *   **Example:** Google and other search engines use complex ranking algorithms that can incorporate gradient boosting techniques.\n",
       "*   **Ad Click-Through Rate (CTR) Prediction:** Estimating the probability that a user will click on an online advertisement. This is crucial for targeted advertising.\n",
       "    *   **Example:** Online advertising platforms use XGBoost to optimize ad placement and targeting.\n",
       "*   **Spam Detection:** Identifying and filtering out unwanted emails or messages.\n",
       "    *   **Example:** Email providers use XGBoost to classify emails as spam or not spam.\n",
       "\n",
       "### 5. Other Industries\n",
       "\n",
       "*   **Manufacturing:** Predictive maintenance, identifying potential equipment failures before they occur.\n",
       "    *   **Example:** Factories use XGBoost to predict when a machine is likely to break down, allowing for proactive maintenance.\n",
       "*   **Transportation:** Optimizing logistics, predicting traffic patterns, and improving route planning.\n",
       "    *   **Example:** Ride-sharing services use XGBoost to predict demand in different areas and optimize driver allocation.\n",
       "*   **Energy:** Predicting energy consumption, optimizing grid management, and forecasting renewable energy generation.\n",
       "    *   **Example:** Energy companies use XGBoost to predict electricity demand.\n",
       "\n",
       "## Why is XGBoost So Popular?\n",
       "\n",
       "*   **High Accuracy:** It consistently achieves state-of-the-art results on many tabular datasets.\n",
       "*   **Speed and Performance:** Its optimizations make it significantly faster than many other gradient boosting implementations.\n",
       "*   **Robustness:** Its regularization techniques and handling of missing values make it less prone to overfitting and more resilient to noisy data.\n",
       "*   **Flexibility:** It can be used for both classification and regression tasks.\n",
       "*   **Scalability:** It can handle large datasets efficiently.\n",
       "*   **Community Support:** It has a large and active community, meaning plenty of resources, tutorials, and support are available.\n",
       "\n",
       "In summary, XGBoost is a powerful and efficient machine learning algorithm that has revolutionized how we approach many predictive modeling tasks. Its ability to deliver high accuracy with speed and robustness makes it an invaluable tool for data scientists and businesses across a wide spectrum of applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top-p value 0.2, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's break down XGBoost, a powerful and widely used machine learning algorithm, and then explore its real-life applications.\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "XGBoost stands for **eXtreme Gradient Boosting**. At its core, it's an implementation of the **gradient boosting** algorithm, but with significant optimizations and enhancements that make it incredibly fast and accurate.\n",
       "\n",
       "To understand XGBoost, let's first understand its building blocks:\n",
       "\n",
       "1.  **Ensemble Learning:** XGBoost is an ensemble method. This means it combines multiple \"weak\" learners (typically decision trees) to create a single, strong learner. The idea is that by combining many simple models, you can achieve better performance than any single model could on its own.\n",
       "\n",
       "2.  **Boosting:** Boosting is a specific type of ensemble learning where models are built sequentially. Each new model tries to correct the errors made by the previous models. It's like a team of students learning a subject: the first student learns, the second student learns from the first student's mistakes, and so on.\n",
       "\n",
       "3.  **Gradient Boosting:** This is where the \"gradient\" in XGBoost comes in. Gradient boosting works by minimizing a loss function (which measures how bad your model's predictions are). It does this by iteratively adding new models that are trained to predict the *residuals* (the errors) of the previous models. The \"gradient\" refers to the direction of steepest descent in the loss function, guiding the process of error correction.\n",
       "\n",
       "4.  **XGBoost's \"eXtreme\" Enhancements:** This is what sets XGBoost apart. It takes the core gradient boosting idea and makes it \"extreme\" through several key innovations:\n",
       "\n",
       "    *   **Regularization:** XGBoost incorporates L1 (Lasso) and L2 (Ridge) regularization directly into the objective function. This helps prevent overfitting by penalizing complex models, leading to better generalization on unseen data.\n",
       "    *   **Tree Pruning:** Unlike traditional gradient boosting that might grow trees to their full depth and then prune, XGBoost uses a \"depth-first\" approach and prunes trees based on a \"gain\" threshold. If splitting a node doesn't improve the objective function by a certain amount, it stops splitting.\n",
       "    *   **Handling Missing Values:** XGBoost has a built-in mechanism to handle missing values in the data. It learns the best direction to send instances with missing values during the tree splitting process.\n",
       "    *   **Parallel Processing:** XGBoost can leverage multiple CPU cores to speed up training. It can parallelize the process of finding the best split points for each tree.\n",
       "    *   **Cache-Aware Access:** It's designed to efficiently utilize CPU cache, further boosting performance.\n",
       "    *   **Out-of-Core Computation:** For datasets that don't fit into memory, XGBoost can handle them by processing data in chunks.\n",
       "    *   **Cross-Validation:** XGBoost has a built-in cross-validation function, making it easier to tune hyperparameters and assess model performance.\n",
       "\n",
       "**In essence, XGBoost is a highly optimized and regularized gradient boosting algorithm that is known for its speed, accuracy, and robustness.**\n",
       "\n",
       "## How it Works (Simplified Analogy)\n",
       "\n",
       "Imagine you're trying to predict the price of a house.\n",
       "\n",
       "1.  **Initial Guess:** You start with a very simple model, maybe just predicting the average price of all houses. This is your first \"weak learner.\"\n",
       "2.  **Identify Errors:** You look at the houses where your initial guess was wrong (the residuals). Some houses were predicted too low, others too high.\n",
       "3.  **Build a Correction Model:** You train a new, slightly more complex model (another decision tree) to predict these errors. For example, if your initial guess was too low for houses with many bedrooms, this new model will learn to add a \"bonus\" for more bedrooms.\n",
       "4.  **Combine and Refine:** You combine your initial guess with the predictions of the correction model. This gives you a better overall prediction.\n",
       "5.  **Repeat:** You repeat steps 2-4 many times. Each new model focuses on correcting the remaining errors of the combined previous models.\n",
       "6.  **XGBoost's Magic:** XGBoost does this very efficiently. It uses regularization to ensure the correction models don't become too complex and overfit. It also uses clever techniques to speed up the process of finding the best ways to correct errors.\n",
       "\n",
       "## Real-Life Use Cases of XGBoost\n",
       "\n",
       "XGBoost's power and versatility have made it a go-to algorithm for a wide range of problems across various industries. Here are some prominent real-life use cases:\n",
       "\n",
       "### 1. Finance\n",
       "\n",
       "*   **Credit Risk Assessment:** Predicting the likelihood of a borrower defaulting on a loan. XGBoost can analyze a vast number of features (income, credit history, loan amount, etc.) to build highly accurate risk models.\n",
       "    *   **Example:** Banks use XGBoost to decide whether to approve loan applications and at what interest rate.\n",
       "*   **Fraud Detection:** Identifying fraudulent transactions in credit card usage, insurance claims, or online activities. XGBoost can detect subtle patterns that might indicate fraudulent behavior.\n",
       "    *   **Example:** E-commerce platforms use XGBoost to flag suspicious transactions in real-time.\n",
       "*   **Algorithmic Trading:** Developing strategies for buying and selling financial assets based on market data. XGBoost can predict stock price movements or identify trading opportunities.\n",
       "    *   **Example:** Hedge funds use XGBoost to build predictive models for trading strategies.\n",
       "*   **Customer Churn Prediction:** Identifying customers who are likely to stop using a service. This allows companies to proactively offer incentives or address issues to retain them.\n",
       "    *   **Example:** Telecom companies use XGBoost to predict which customers are likely to switch providers.\n",
       "\n",
       "### 2. E-commerce and Retail\n",
       "\n",
       "*   **Recommendation Systems:** Suggesting products or content to users based on their past behavior and preferences. XGBoost can power sophisticated recommendation engines.\n",
       "    *   **Example:** Online retailers like Amazon use XGBoost to recommend products you might like.\n",
       "*   **Sales Forecasting:** Predicting future sales volumes for products or stores. This helps with inventory management, staffing, and marketing.\n",
       "    *   **Example:** Retailers use XGBoost to forecast demand for seasonal products.\n",
       "*   **Customer Lifetime Value (CLV) Prediction:** Estimating the total revenue a customer is expected to generate over their relationship with a company.\n",
       "    *   **Example:** Marketing teams use CLV predictions to allocate marketing budgets effectively.\n",
       "\n",
       "### 3. Healthcare\n",
       "\n",
       "*   **Disease Prediction and Diagnosis:** Identifying patients at risk of developing certain diseases or assisting in the diagnosis of medical conditions based on patient data.\n",
       "    *   **Example:** Researchers use XGBoost to predict the likelihood of a patient developing diabetes based on their medical history and lifestyle.\n",
       "*   **Drug Discovery:** Analyzing biological data to identify potential drug candidates or predict the efficacy of new treatments.\n",
       "    *   **Example:** Pharmaceutical companies use XGBoost to screen potential drug compounds.\n",
       "*   **Patient Readmission Prediction:** Identifying patients who are at high risk of being readmitted to the hospital shortly after discharge.\n",
       "    *   **Example:** Hospitals use XGBoost to identify patients who need extra post-discharge care.\n",
       "\n",
       "### 4. Technology and Internet\n",
       "\n",
       "*   **Search Engine Ranking:** Improving the relevance of search results by predicting which pages are most likely to satisfy a user's query.\n",
       "    *   **Example:** Google and other search engines use complex ranking algorithms that can incorporate gradient boosting techniques.\n",
       "*   **Ad Click-Through Rate (CTR) Prediction:** Estimating the probability that a user will click on an online advertisement. This is crucial for targeted advertising.\n",
       "    *   **Example:** Online advertising platforms use XGBoost to optimize ad placement and targeting.\n",
       "*   **Spam Detection:** Identifying and filtering out unwanted emails or messages.\n",
       "    *   **Example:** Email providers use XGBoost to classify emails as spam or not spam.\n",
       "\n",
       "### 5. Other Industries\n",
       "\n",
       "*   **Manufacturing:** Predictive maintenance, identifying potential equipment failures before they occur.\n",
       "    *   **Example:** Factories use XGBoost to predict when a machine is likely to break down, allowing for proactive maintenance.\n",
       "*   **Transportation:** Optimizing logistics, predicting traffic patterns, and improving route planning.\n",
       "    *   **Example:** Ride-sharing services use XGBoost to predict demand in different areas and optimize driver allocation.\n",
       "*   **Energy:** Predicting energy consumption, optimizing grid management, and forecasting renewable energy generation.\n",
       "    *   **Example:** Energy companies use XGBoost to predict electricity demand.\n",
       "\n",
       "## Why is XGBoost So Popular?\n",
       "\n",
       "*   **High Accuracy:** It consistently achieves state-of-the-art results on many tabular datasets.\n",
       "*   **Speed and Performance:** Its optimizations make it significantly faster than many other gradient boosting implementations.\n",
       "*   **Robustness:** Its regularization techniques and handling of missing values make it less prone to overfitting and more resilient to noisy data.\n",
       "*   **Flexibility:** It can be used for both classification and regression tasks.\n",
       "*   **Scalability:** It can handle large datasets efficiently.\n",
       "*   **Community Support:** It has a large and active community, meaning plenty of resources, tutorials, and support are available.\n",
       "\n",
       "In summary, XGBoost is a powerful and efficient machine learning algorithm that has revolutionized how we approach many predictive modeling tasks. Its ability to deliver high accuracy with speed and robustness makes it an invaluable tool for data scientists and businesses across a wide spectrum of applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top-p value 0.4, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's break down XGBoost, a powerful and widely used machine learning algorithm, and then explore its real-life applications.\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "XGBoost stands for **eXtreme Gradient Boosting**. At its core, it's an implementation of the **gradient boosting** algorithm, but with significant optimizations and enhancements that make it incredibly fast and accurate.\n",
       "\n",
       "To understand XGBoost, let's first understand its building blocks:\n",
       "\n",
       "1.  **Ensemble Learning:** XGBoost is an ensemble method. This means it combines multiple \"weak\" learners (typically decision trees) to create a single, strong learner. The idea is that by combining many simple models, you can achieve better performance than any single model could on its own.\n",
       "\n",
       "2.  **Boosting:** Boosting is a specific type of ensemble learning where models are built sequentially. Each new model tries to correct the errors made by the previous models.\n",
       "    *   Imagine you're trying to predict house prices.\n",
       "    *   **Model 1:** Might be a simple decision tree that makes some reasonable predictions but misses many nuances.\n",
       "    *   **Model 2:** Is trained to focus on the houses that Model 1 got wrong. It learns from the \"residuals\" (the difference between the actual price and Model 1's prediction).\n",
       "    *   **Model 3:** Focuses on the errors of Model 1 and Model 2 combined.\n",
       "    *   This process continues, with each new tree learning from the mistakes of the ensemble built so far.\n",
       "\n",
       "3.  **Gradient Boosting:** This is where the \"gradient\" in XGBoost comes in. Instead of directly correcting errors, gradient boosting uses the concept of gradients from calculus.\n",
       "    *   Think of it like finding the lowest point in a valley. You take steps in the direction of the steepest descent (the negative gradient).\n",
       "    *   In gradient boosting, each new tree is trained to predict the **negative gradient** of the loss function with respect to the predictions of the current ensemble. This is a more sophisticated way of saying it's trying to minimize the overall error.\n",
       "\n",
       "4.  **XGBoost's \"eXtreme\" Enhancements:** This is what sets XGBoost apart. It takes the core gradient boosting idea and makes it much more efficient and robust:\n",
       "\n",
       "    *   **Regularization:** XGBoost includes L1 (Lasso) and L2 (Ridge) regularization terms in its objective function. This helps prevent overfitting by penalizing complex models. It essentially discourages trees from becoming too deep or having too many branches.\n",
       "    *   **Parallel Processing:** While boosting is sequential, XGBoost can parallelize the tree building process. It can process features in parallel, significantly speeding up training.\n",
       "    *   **Handling Missing Values:** XGBoost has a built-in mechanism to handle missing values in the data. It learns the best direction to go when a value is missing.\n",
       "    *   **Tree Pruning:** It uses a more advanced pruning technique than traditional gradient boosting, which helps in building more accurate trees.\n",
       "    *   **Cache-Aware Access:** It's designed to efficiently utilize CPU cache, further boosting performance.\n",
       "    *   **Out-of-Core Computation:** For very large datasets that don't fit into memory, XGBoost can handle them by processing data in chunks.\n",
       "    *   **Sparsity Awareness:** It can efficiently handle sparse data (data with many zero values).\n",
       "\n",
       "**In essence, XGBoost is a highly optimized and regularized gradient boosting algorithm that builds an ensemble of decision trees sequentially, with each new tree correcting the errors of the previous ones, leading to exceptional predictive accuracy and speed.**\n",
       "\n",
       "## Real-Life Use Cases of XGBoost\n",
       "\n",
       "XGBoost's power and versatility have made it a go-to algorithm for a wide range of problems across various industries. Here are some prominent real-life use cases:\n",
       "\n",
       "### 1. Finance and Banking\n",
       "\n",
       "*   **Credit Risk Assessment:** Predicting the likelihood of a customer defaulting on a loan. XGBoost can analyze a vast number of features (income, credit history, debt-to-income ratio, etc.) to build highly accurate risk models.\n",
       "    *   **Example:** A bank uses XGBoost to score loan applications. The model identifies applicants with a high probability of default, allowing the bank to deny them or offer them different loan terms, thus minimizing financial losses.\n",
       "*   **Fraud Detection:** Identifying fraudulent transactions (e.g., credit card fraud, insurance claims). XGBoost can detect subtle patterns in transaction data that indicate fraudulent activity.\n",
       "    *   **Example:** An e-commerce platform uses XGBoost to flag suspicious transactions in real-time. If a transaction deviates significantly from a user's typical spending behavior, it's flagged for review, preventing financial loss.\n",
       "*   **Algorithmic Trading:** Developing trading strategies based on market data. XGBoost can predict stock price movements or identify trading opportunities.\n",
       "    *   **Example:** A hedge fund uses XGBoost to predict the short-term price movements of stocks based on historical price data, news sentiment, and economic indicators, informing their automated trading decisions.\n",
       "\n",
       "### 2. E-commerce and Retail\n",
       "\n",
       "*   **Customer Churn Prediction:** Identifying customers who are likely to stop using a service or buying products. This allows businesses to proactively engage with these customers to retain them.\n",
       "    *   **Example:** A subscription service uses XGBoost to predict which customers are at risk of canceling their subscription. They then offer targeted discounts or personalized recommendations to these customers to improve retention rates.\n",
       "*   **Recommendation Systems:** Suggesting products or content to users based on their past behavior and preferences. While not always the primary algorithm, XGBoost can be used to rank recommendations or as part of a hybrid system.\n",
       "    *   **Example:** An online streaming service uses XGBoost to predict which movies a user is most likely to enjoy based on their viewing history, ratings, and the behavior of similar users.\n",
       "*   **Sales Forecasting:** Predicting future sales volumes for products or stores. This helps with inventory management, staffing, and marketing planning.\n",
       "    *   **Example:** A retail chain uses XGBoost to forecast sales for each of its stores for the next quarter. This helps them optimize inventory levels, ensuring they have enough stock without overstocking.\n",
       "\n",
       "### 3. Healthcare\n",
       "\n",
       "*   **Disease Prediction and Diagnosis:** Predicting the likelihood of a patient developing a certain disease or assisting in diagnosis. XGBoost can analyze patient medical history, genetic data, and lifestyle factors.\n",
       "    *   **Example:** Researchers use XGBoost to predict the risk of a patient developing diabetes based on their age, BMI, family history, and blood test results. This allows for early intervention and preventative measures.\n",
       "*   **Drug Discovery and Development:** Identifying potential drug candidates or predicting the efficacy of existing drugs.\n",
       "    *   **Example:** Pharmaceutical companies use XGBoost to analyze vast datasets of molecular structures and biological interactions to predict which compounds are most likely to be effective against a specific disease.\n",
       "*   **Patient Readmission Prediction:** Identifying patients who are at high risk of being readmitted to the hospital shortly after discharge. This allows hospitals to provide targeted post-discharge care.\n",
       "    *   **Example:** A hospital uses XGBoost to identify patients who are likely to be readmitted within 30 days of discharge. They then assign these patients to a special care management program to reduce readmission rates.\n",
       "\n",
       "### 4. Technology and Internet\n",
       "\n",
       "*   **Search Engine Ranking:** While complex, elements of ranking algorithms can utilize gradient boosting techniques to improve search result relevance.\n",
       "*   **Ad Click-Through Rate (CTR) Prediction:** Predicting the probability that a user will click on an advertisement. This is crucial for online advertising platforms.\n",
       "    *   **Example:** An online advertising platform uses XGBoost to predict the likelihood of a user clicking on a specific ad based on the ad's content, the user's browsing history, and demographic information. This optimizes ad placement and revenue.\n",
       "*   **Spam Detection:** Identifying and filtering out spam emails or messages.\n",
       "    *   **Example:** Email providers use XGBoost to classify incoming emails as either legitimate or spam based on various features like sender reputation, email content, and presence of suspicious links.\n",
       "\n",
       "### 5. Other Industries\n",
       "\n",
       "*   **Manufacturing:** Predictive maintenance (predicting when machinery is likely to fail) and quality control.\n",
       "*   **Energy:** Predicting energy consumption or renewable energy generation.\n",
       "*   **Transportation:** Optimizing logistics, predicting traffic patterns, or forecasting demand for ride-sharing services.\n",
       "*   **Sports Analytics:** Predicting game outcomes, player performance, or identifying talent.\n",
       "\n",
       "**Why is XGBoost so popular?**\n",
       "\n",
       "*   **High Accuracy:** It consistently achieves state-of-the-art results on many tabular datasets.\n",
       "*   **Speed and Performance:** Its optimized implementation makes it significantly faster than many other gradient boosting libraries.\n",
       "*   **Flexibility:** It can be used for both classification and regression tasks.\n",
       "*   **Robustness:** Its regularization techniques help prevent overfitting, making it more reliable on unseen data.\n",
       "*   **Scalability:** It can handle large datasets efficiently.\n",
       "\n",
       "In summary, XGBoost is a powerful and versatile machine learning algorithm that has revolutionized how we approach predictive modeling. Its ability to handle complex relationships in data, combined with its speed and robustness, makes it an invaluable tool for solving a wide array of real-world problems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top-p value 0.8, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's break down XGBoost (Extreme Gradient Boosting) and explore its real-life applications.\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "XGBoost is a powerful and highly efficient open-source implementation of the **gradient boosting** algorithm. It's renowned for its speed, accuracy, and ability to handle complex datasets. To understand XGBoost, we first need to grasp the core idea of gradient boosting.\n",
       "\n",
       "**Gradient Boosting in a Nutshell:**\n",
       "\n",
       "Imagine you're trying to predict something, like house prices. Gradient boosting works by building a series of weak predictive models (usually decision trees) sequentially.\n",
       "\n",
       "1.  **First Tree:** The first tree makes an initial prediction. It's likely to be wrong, but it captures some of the underlying patterns.\n",
       "2.  **Second Tree (Focus on Errors):** The second tree is then trained to predict the *errors* or *residuals* of the first tree. It focuses on the cases where the first tree was most wrong.\n",
       "3.  **Third Tree (Focus on New Errors):** The third tree is trained to predict the errors of the *combined* predictions of the first two trees.\n",
       "4.  **And So On...** This process continues, with each new tree trying to correct the mistakes of the ensemble of previous trees.\n",
       "\n",
       "The key idea is that by iteratively adding trees that focus on the remaining errors, the ensemble gradually improves its overall prediction accuracy.\n",
       "\n",
       "**What Makes XGBoost \"Extreme\"?**\n",
       "\n",
       "XGBoost takes gradient boosting to the next level with several key optimizations and features:\n",
       "\n",
       "*   **Regularization:** XGBoost incorporates L1 (Lasso) and L2 (Ridge) regularization directly into the objective function. This helps prevent overfitting by penalizing complex models, leading to better generalization on unseen data.\n",
       "*   **Parallel Processing:** It can utilize multiple CPU cores to speed up the tree building process, making it significantly faster than traditional gradient boosting implementations.\n",
       "*   **Tree Pruning:** XGBoost uses a more advanced pruning technique than standard decision trees. It builds trees to a certain depth and then prunes them back based on a gain threshold, preventing overly complex trees.\n",
       "*   **Handling Missing Values:** XGBoost has a built-in mechanism to handle missing values in the data, intelligently learning the best direction to go when a value is absent.\n",
       "*   **Cache-Aware Access:** It's designed to efficiently utilize CPU cache, further boosting performance.\n",
       "*   **Out-of-Core Computation:** For datasets that don't fit into memory, XGBoost can process them in chunks, allowing it to handle very large datasets.\n",
       "*   **More Sophisticated Objective Function:** It uses a second-order Taylor expansion of the loss function, which provides a more accurate approximation of the error and leads to better convergence.\n",
       "\n",
       "**In essence, XGBoost is a highly optimized, regularized, and scalable gradient boosting framework.**\n",
       "\n",
       "## Real-Life Use Cases of XGBoost\n",
       "\n",
       "XGBoost's power and versatility have made it a go-to algorithm for a wide range of machine learning tasks across various industries. Here are some prominent real-life use cases:\n",
       "\n",
       "### 1. Financial Services\n",
       "\n",
       "*   **Credit Risk Assessment:** Predicting the likelihood of a customer defaulting on a loan. XGBoost can analyze numerous customer attributes (income, credit history, debt-to-income ratio, etc.) to build highly accurate models for risk scoring.\n",
       "*   **Fraud Detection:** Identifying fraudulent transactions (e.g., credit card fraud, insurance claims). XGBoost can detect subtle patterns and anomalies that might indicate fraudulent activity.\n",
       "*   **Algorithmic Trading:** Developing strategies for buying and selling financial assets based on market data. XGBoost can predict price movements or identify trading opportunities.\n",
       "*   **Customer Churn Prediction:** Identifying customers who are likely to leave a service (e.g., a bank, an insurance company). This allows companies to proactively engage these customers with retention offers.\n",
       "\n",
       "**Example:** A bank uses XGBoost to predict which loan applicants are most likely to default. By analyzing historical data, the model can identify patterns associated with defaults and flag high-risk applicants, allowing the bank to make more informed lending decisions.\n",
       "\n",
       "### 2. E-commerce and Retail\n",
       "\n",
       "*   **Recommendation Systems:** Suggesting products to customers based on their past purchases, browsing history, and the behavior of similar users. XGBoost can learn complex user preferences.\n",
       "*   **Sales Forecasting:** Predicting future sales volumes for products or stores. This helps with inventory management, staffing, and marketing planning.\n",
       "*   **Customer Lifetime Value (CLV) Prediction:** Estimating the total revenue a customer is expected to generate over their relationship with the company. This informs marketing spend and customer segmentation.\n",
       "*   **Personalized Marketing:** Tailoring marketing campaigns and offers to individual customers based on their predicted behavior and preferences.\n",
       "\n",
       "**Example:** An online retailer uses XGBoost to power its \"Customers who bought this also bought...\" feature. By analyzing purchase patterns, the model can recommend relevant products, increasing cross-selling and customer engagement.\n",
       "\n",
       "### 3. Healthcare\n",
       "\n",
       "*   **Disease Prediction and Diagnosis:** Identifying patients at risk of developing certain diseases (e.g., diabetes, heart disease) or assisting in the diagnosis of conditions based on medical records and symptoms.\n",
       "*   **Drug Discovery and Development:** Predicting the efficacy and potential side effects of new drug compounds.\n",
       "*   **Patient Readmission Prediction:** Identifying patients who are at high risk of being readmitted to the hospital after discharge, allowing for targeted post-discharge care.\n",
       "*   **Medical Image Analysis:** While deep learning is dominant here, XGBoost can be used for feature extraction and classification in conjunction with other methods.\n",
       "\n",
       "**Example:** A hospital uses XGBoost to predict which patients are at highest risk of developing sepsis after surgery. Early identification allows for timely intervention and improved patient outcomes.\n",
       "\n",
       "### 4. Marketing and Advertising\n",
       "\n",
       "*   **Click-Through Rate (CTR) Prediction:** Estimating the probability that a user will click on an advertisement. This is crucial for online advertising platforms to optimize ad placement and targeting.\n",
       "*   **Lead Scoring:** Ranking potential customers (leads) based on their likelihood to convert into paying customers.\n",
       "*   **Campaign Performance Optimization:** Predicting the success of marketing campaigns and identifying factors that contribute to higher conversion rates.\n",
       "\n",
       "**Example:** An online advertising platform uses XGBoost to predict the CTR for ads shown to users. This helps them serve more relevant ads, leading to higher engagement and revenue for advertisers.\n",
       "\n",
       "### 5. Telecommunications\n",
       "\n",
       "*   **Customer Churn Prediction:** Identifying customers who are likely to switch to a competitor. This allows telecom companies to offer incentives or address service issues to retain customers.\n",
       "*   **Network Anomaly Detection:** Identifying unusual patterns in network traffic that could indicate outages, security breaches, or performance issues.\n",
       "\n",
       "**Example:** A mobile carrier uses XGBoost to predict which customers are likely to cancel their subscriptions. They can then offer these customers special plans or discounts to encourage them to stay.\n",
       "\n",
       "### 6. Manufacturing and Industrial Applications\n",
       "\n",
       "*   **Predictive Maintenance:** Predicting when machinery is likely to fail, allowing for proactive maintenance and preventing costly downtime.\n",
       "*   **Quality Control:** Identifying defects in manufactured products based on sensor data or inspection results.\n",
       "*   **Supply Chain Optimization:** Predicting demand and optimizing inventory levels to reduce costs and improve efficiency.\n",
       "\n",
       "**Example:** A manufacturing plant uses XGBoost to predict when a critical piece of equipment is likely to break down based on sensor readings (temperature, vibration, etc.). This allows them to schedule maintenance before a failure occurs, avoiding production halts.\n",
       "\n",
       "### 7. Other Areas\n",
       "\n",
       "*   **Natural Language Processing (NLP):** While deep learning models like Transformers are more common for complex NLP tasks, XGBoost can be used for text classification, sentiment analysis, and spam detection, especially when combined with feature engineering.\n",
       "*   **Image Classification (Feature-based):** For datasets where feature extraction is done manually or by other algorithms, XGBoost can be used for classification.\n",
       "*   **Sports Analytics:** Predicting game outcomes, player performance, or identifying winning strategies.\n",
       "\n",
       "**In summary, XGBoost is a versatile and powerful algorithm that excels in predictive modeling tasks. Its ability to handle complex relationships, prevent overfitting, and its computational efficiency make it a top choice for data scientists and engineers tackling real-world problems across numerous domains.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top-p value 1, result: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let's break down XGBoost (Extreme Gradient Boosting) and explore its power with real-life examples.\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "At its core, XGBoost is a powerful and highly efficient **gradient boosting algorithm**. To understand XGBoost, we first need to understand the concept of **boosting**.\n",
       "\n",
       "**Boosting: Building a Stronger Learner from Weak Ones**\n",
       "\n",
       "Imagine you want to build a very accurate prediction model. Instead of creating one complex, super-intelligent model from scratch, boosting takes a different approach:\n",
       "\n",
       "1.  **Ensemble of Weak Learners:** It builds a large number of simpler models (often called \"weak learners\"). Decision trees are the most common weak learners used in XGBoost.\n",
       "2.  **Sequential Learning:** These weak learners are built sequentially, one after another.\n",
       "3.  **Error Correction:** Each new weak learner focuses on correcting the errors made by the previously built learners. It pays more attention to the data points that were misclassified or had large prediction errors.\n",
       "4.  **Weighted Voting:** Finally, the predictions of all the weak learners are combined (usually with weights) to produce a single, highly accurate prediction.\n",
       "\n",
       "Think of it like a team of students working on a complex problem. The first student tries their best, but inevitably makes some mistakes. The second student reviews the first student's work, identifies the errors, and tries to fix them. This process continues, with each subsequent student learning from the mistakes of their predecessors, leading to a much better overall solution.\n",
       "\n",
       "**Gradient Boosting: The \"Gradient\" Part**\n",
       "\n",
       "The \"gradient\" in gradient boosting refers to a mathematical optimization technique. In essence, it uses the **gradient of the loss function** to guide the learning process. The loss function measures how bad the model's current predictions are. By calculating the gradient of this loss function, the algorithm knows in which direction and how much it needs to adjust the weak learners to minimize the errors.\n",
       "\n",
       "**XGBoost: The \"Extreme\" Evolution**\n",
       "\n",
       "XGBoost takes the gradient boosting framework and adds several key optimizations and features that make it incredibly powerful, fast, and accurate:\n",
       "\n",
       "*   **Regularization:** XGBoost incorporates L1 (Lasso) and L2 (Ridge) regularization techniques. This helps prevent overfitting by penalizing complex models, leading to better generalization on unseen data.\n",
       "*   **Parallelization:** It can leverage multiple CPU cores to speed up training, making it much faster than traditional gradient boosting implementations.\n",
       "*   **Handling Missing Values:** XGBoost has built-in mechanisms to handle missing values in the dataset, which is a common challenge in real-world data.\n",
       "*   **Tree Pruning:** It employs advanced tree pruning techniques to control tree complexity and avoid overfitting.\n",
       "*   **Cache-Aware Access:** XGBoost is designed to efficiently utilize CPU caches, further boosting performance.\n",
       "*   **Out-of-Core Computation:** It can handle datasets that are too large to fit into memory, by processing them in chunks.\n",
       "\n",
       "**In summary, XGBoost is a highly optimized and regularized gradient boosting algorithm that uses an ensemble of decision trees to make accurate predictions by sequentially learning from the errors of previous trees.**\n",
       "\n",
       "## Real-Life Use Cases of XGBoost\n",
       "\n",
       "XGBoost's versatility, accuracy, and speed have made it a go-to algorithm for a wide range of applications across various industries. Here are some prominent real-life use cases:\n",
       "\n",
       "**1. E-commerce and Retail:**\n",
       "\n",
       "*   **Product Recommendation:** XGBoost can analyze user purchase history, browsing behavior, and product attributes to recommend personalized products, increasing sales and customer engagement.\n",
       "    *   **Example:** Amazon uses similar ensemble methods to suggest \"Customers who bought this item also bought...\" or tailor recommendations based on your past interactions.\n",
       "*   **Demand Forecasting:** Predicting the demand for products is crucial for inventory management and supply chain optimization. XGBoost can forecast demand based on historical sales, seasonality, promotions, and other relevant factors.\n",
       "    *   **Example:** A clothing retailer can use XGBoost to predict how many units of a specific jacket will be sold in the next month, considering factors like upcoming holidays, weather forecasts, and marketing campaigns.\n",
       "*   **Customer Churn Prediction:** Identifying customers who are likely to stop using a service or product allows companies to proactively engage them with special offers or improved service.\n",
       "    *   **Example:** A mobile carrier can use XGBoost to predict which customers are at high risk of switching to a competitor, allowing them to offer retention incentives.\n",
       "\n",
       "**2. Finance:**\n",
       "\n",
       "*   **Credit Scoring and Risk Assessment:** XGBoost can analyze vast amounts of financial data to assess the creditworthiness of individuals or businesses and predict the likelihood of loan default.\n",
       "    *   **Example:** Banks use XGBoost to determine whether to approve a loan application by evaluating factors like income, credit history, debt-to-income ratio, and employment stability.\n",
       "*   **Fraud Detection:** Identifying fraudulent transactions or activities is critical for financial institutions. XGBoost can learn patterns associated with fraudulent behavior and flag suspicious transactions in real-time.\n",
       "    *   **Example:** A credit card company can use XGBoost to detect unusual spending patterns or transactions occurring in unusual locations, potentially indicating a stolen card.\n",
       "*   **Algorithmic Trading:** While complex, simplified trading strategies can be built using XGBoost to predict stock price movements or identify profitable trading opportunities based on historical market data.\n",
       "    *   **Example:** An investment firm might use XGBoost to predict the short-term price movement of a stock based on its historical performance, news sentiment, and economic indicators.\n",
       "\n",
       "**3. Healthcare:**\n",
       "\n",
       "*   **Disease Prediction and Diagnosis:** XGBoost can analyze patient data (medical history, lab results, genetic information) to predict the likelihood of developing certain diseases or to assist in diagnosis.\n",
       "    *   **Example:** A hospital can use XGBoost to predict a patient's risk of developing diabetes based on their age, weight, family history, and lifestyle factors.\n",
       "*   **Drug Discovery and Development:** Identifying potential drug candidates or predicting the efficacy of existing drugs can be accelerated using XGBoost on large biological and chemical datasets.\n",
       "    *   **Example:** Pharmaceutical companies can use XGBoost to analyze the effectiveness of different drug compounds in targeting specific diseases.\n",
       "*   **Patient Readmission Prediction:** Predicting which patients are likely to be readmitted to the hospital soon after discharge allows healthcare providers to offer targeted post-discharge care.\n",
       "    *   **Example:** A hospital can identify patients at high risk of readmission after a heart attack and provide them with extra support and follow-up care.\n",
       "\n",
       "**4. Marketing and Advertising:**\n",
       "\n",
       "*   **Click-Through Rate (CTR) Prediction:** In online advertising, predicting which ads users are most likely to click on is crucial for campaign efficiency. XGBoost excels at this by analyzing user demographics, browsing history, and ad context.\n",
       "    *   **Example:** Google Ads uses sophisticated machine learning models, including gradient boosting, to determine which ads to show to users to maximize clicks and conversions.\n",
       "*   **Customer Segmentation:** Grouping customers into distinct segments based on their behavior, demographics, and preferences allows for more targeted marketing campaigns.\n",
       "    *   **Example:** A marketing team can use XGBoost to identify different customer segments (e.g., \"loyal high-spenders,\" \"price-sensitive shoppers,\" \"new prospects\") to tailor their messaging.\n",
       "*   **Lead Scoring:** Identifying which sales leads are most likely to convert into paying customers helps sales teams prioritize their efforts.\n",
       "    *   **Example:** A software company can use XGBoost to score leads based on their website activity, engagement with marketing materials, and company size.\n",
       "\n",
       "**5. Other Industries:**\n",
       "\n",
       "*   **Image Recognition and Computer Vision:** While deep learning models like Convolutional Neural Networks (CNNs) are dominant here, XGBoost can be used for feature extraction and classification tasks, especially when dealing with structured or tabular data derived from images.\n",
       "*   **Natural Language Processing (NLP):** For tasks involving structured text data or feature engineering from text, XGBoost can be effective in areas like sentiment analysis or document classification.\n",
       "*   **Manufacturing:** Predicting equipment failures, optimizing production processes, and quality control.\n",
       "    *   **Example:** A factory can use XGBoost to predict when a machine is likely to break down based on its operating conditions and maintenance history.\n",
       "*   **Transportation:** Optimizing logistics, predicting delivery times, and analyzing traffic patterns.\n",
       "    *   **Example:** A logistics company could use XGBoost to predict the arrival time of a delivery truck, taking into account real-time traffic data, weather conditions, and estimated loading times.\n",
       "\n",
       "**Why is XGBoost so popular?**\n",
       "\n",
       "*   **High Accuracy:** Consistently achieves state-of-the-art results on many machine learning tasks.\n",
       "*   **Speed and Efficiency:** Its optimized implementation makes it faster than many other gradient boosting libraries.\n",
       "*   **Robustness:** Handles missing values and regularization effectively, preventing overfitting.\n",
       "*   **Scalability:** Can handle large datasets efficiently.\n",
       "*   **Flexibility:** Can be used for both classification and regression problems.\n",
       "\n",
       "XGBoost has revolutionized how many machine learning problems are solved, providing a powerful and reliable tool for data scientists and engineers alike."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experiment with 'top_p' parameter \n",
    "for p in [0, 0.2, 0.4, 0.8, 1]: \n",
    "    config = genai.types.GenerationConfig(top_p=p) \n",
    "    result = get_response(\"Explain the concept of XGBoost with real-life use cases.\", generation_config=config) \n",
    "\n",
    "    print(f\"\\n\\nFor top-p value {p}, result: \\n\\n\") \n",
    "    display(Markdown(result.text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e92b01e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's dive into XGBoost, a powerful and widely used machine learning algorithm, and explore its real-world applications.\n",
       "\n",
       "## What is XGBoost?\n",
       "\n",
       "XGBoost stands for **Extreme Gradient Boosting**. At its core, it's an implementation of the **gradient boosting** framework, but with significant optimizations and enhancements that make it exceptionally fast and accurate.\n",
       "\n",
       "To understand XGBoost, let's break down its components:\n",
       "\n",
       "1.  **Gradient Boosting:**\n",
       "    *   **Ensemble Method:** Gradient boosting is an ensemble technique, meaning it combines multiple \"weak\" learning models (typically decision trees) to create a \"strong\" predictive model.\n",
       "    *   **Sequential Learning:** Unlike methods like Random Forests where trees are built independently, gradient boosting builds trees sequentially. Each new tree tries to correct the errors made by the previous ones.\n",
       "    *   **Gradient Descent Analogy:** The \"gradient\" part refers to its connection to gradient descent. Imagine you're trying to minimize an error function (like the difference between predicted and actual values). Gradient boosting uses gradient descent to iteratively find the best \"direction\" to improve the model by adding new trees that reduce the residual errors.\n",
       "    *   **Residuals:** If a model makes a prediction, the \"residual\" is the difference between the actual value and the predicted value. Subsequent trees are trained on these residuals, effectively learning to predict the remaining errors.\n",
       "\n",
       "2.  **Extreme Enhancements:** XGBoost takes this sequential learning process and injects several key improvements:\n",
       "    *   **Regularization:** XGBoost incorporates L1 (Lasso) and L2 (Ridge) regularization. This helps prevent overfitting by penalizing complex models, leading to better generalization on unseen data.\n",
       "    *   **Handling Missing Values:** XGBoost has a built-in mechanism to handle missing data. It learns the best direction to go for missing values within the tree structure, rather than simply imputing them beforehand.\n",
       "    *   **Tree Pruning:** While standard gradient boosting can grow trees deep and then prune them, XGBoost uses a \"depth-first\" approach where it prunes branches that don't contribute to reducing the loss, often leading to more efficient and accurate trees.\n",
       "    *   **Parallelization:** XGBoost can parallelize the tree building process, making it significantly faster than many other gradient boosting implementations. It can utilize multiple CPU cores effectively.\n",
       "    *   **Cache-Aware Access:** It optimizes data access patterns to improve hardware utilization and speed up computations.\n",
       "    *   **More Sophisticated Splitting:** It uses techniques like \"exact greedy\" for finding the best split points and can also use approximate algorithms for very large datasets.\n",
       "    *   **Weighted Quantile Sketch:** For handling large datasets, it uses a weighted quantile sketch algorithm to find candidate split points efficiently.\n",
       "\n",
       "**In simpler terms:** XGBoost is like building a team of experts. Each expert (decision tree) is given a task to identify and correct the mistakes made by the team so far. XGBoost is the super-efficient manager that trains these experts quickly, makes sure they don't get too specialized (overfitting), and handles any missing information gracefully.\n",
       "\n",
       "## Why is XGBoost so Popular?\n",
       "\n",
       "*   **High Accuracy:** It consistently achieves state-of-the-art results on many tabular data problems.\n",
       "*   **Speed and Performance:** Its optimizations make it much faster than traditional gradient boosting algorithms.\n",
       "*   **Robustness:** Its built-in regularization and handling of missing values make it less prone to errors and more reliable.\n",
       "*   **Flexibility:** It can be used for both classification and regression tasks.\n",
       "*   **Community Support and Documentation:** It has a large and active community, making it easy to find help and resources.\n",
       "\n",
       "## Real-Life Use Cases of XGBoost:\n",
       "\n",
       "XGBoost's versatility and performance have made it a go-to algorithm in a wide range of industries. Here are some prominent examples:\n",
       "\n",
       "### 1. Finance:\n",
       "\n",
       "*   **Credit Risk Assessment:** Predicting the likelihood of a customer defaulting on a loan. XGBoost can analyze a multitude of factors (income, credit history, debt-to-income ratio, etc.) to assign a risk score.\n",
       "    *   **Real-life Example:** Banks and credit card companies use XGBoost to decide whether to approve a loan application or to set interest rates based on the applicant's risk profile.\n",
       "*   **Fraud Detection:** Identifying fraudulent transactions in credit card payments, insurance claims, or stock trading. XGBoost can spot anomalies and patterns indicative of fraud.\n",
       "    *   **Real-life Example:** Online payment gateways and insurance providers use XGBoost to flag suspicious activities in real-time, preventing financial losses.\n",
       "*   **Algorithmic Trading:** Developing strategies for buying and selling financial instruments based on market predictions.\n",
       "    *   **Real-life Example:** Hedge funds and investment firms use XGBoost to predict stock price movements or identify profitable trading opportunities.\n",
       "*   **Customer Churn Prediction:** Identifying customers who are likely to stop using a financial service (e.g., closing a bank account, canceling an insurance policy).\n",
       "    *   **Real-life Example:** Banks and telecom companies use XGBoost to proactively offer incentives or better service to customers at risk of churning.\n",
       "\n",
       "### 2. E-commerce and Retail:\n",
       "\n",
       "*   **Sales Forecasting:** Predicting future sales volumes for products or categories.\n",
       "    *   **Real-life Example:** Retailers use XGBoost to optimize inventory management, staffing, and marketing campaigns based on predicted demand.\n",
       "*   **Recommendation Systems:** Suggesting products or content to users based on their past behavior and preferences. While not always the primary engine, XGBoost can be used as a powerful ranking component or feature engineering tool.\n",
       "    *   **Real-life Example:** Online marketplaces and streaming services use recommendation engines, and XGBoost can help improve the relevance of suggested items.\n",
       "*   **Price Optimization:** Dynamically adjusting prices based on demand, competitor pricing, and other market factors.\n",
       "    *   **Real-life Example:** Airlines and ride-sharing services use dynamic pricing, and XGBoost can help them find optimal price points.\n",
       "*   **Customer Lifetime Value (CLV) Prediction:** Estimating the total revenue a customer is expected to generate over their relationship with a company.\n",
       "    *   **Real-life Example:** Businesses use CLV predictions to segment customers and tailor marketing efforts, focusing on high-value customers.\n",
       "\n",
       "### 3. Healthcare:\n",
       "\n",
       "*   **Disease Prediction and Diagnosis:** Predicting the likelihood of a patient developing a certain disease based on their medical history, lifestyle, and genetic factors.\n",
       "    *   **Real-life Example:** Hospitals and research institutions use XGBoost to identify individuals at high risk for conditions like diabetes, heart disease, or cancer, allowing for early intervention.\n",
       "*   **Drug Discovery and Development:** Predicting the efficacy and potential side effects of new drugs.\n",
       "    *   **Real-life Example:** Pharmaceutical companies use XGBoost to analyze vast datasets of chemical compounds and biological interactions to accelerate the drug development process.\n",
       "*   **Patient Readmission Prediction:** Identifying patients who are at a high risk of being readmitted to the hospital shortly after discharge.\n",
       "    *   **Real-life Example:** Hospitals use this to provide targeted post-discharge care and support, reducing readmission rates and healthcare costs.\n",
       "\n",
       "### 4. Telecommunications:\n",
       "\n",
       "*   **Customer Churn Prediction:** As mentioned in finance, this is a critical problem for telecom companies. Identifying customers likely to switch providers.\n",
       "    *   **Real-life Example:** Telecom operators use XGBoost to identify at-risk customers and offer retention incentives like discounts or better plans.\n",
       "*   **Network Anomaly Detection:** Identifying unusual patterns in network traffic that might indicate failures, cyberattacks, or performance issues.\n",
       "    *   **Real-life Example:** Telecom infrastructure providers use XGBoost to monitor network health and proactively address potential problems.\n",
       "\n",
       "### 5. Other Industries:\n",
       "\n",
       "*   **Manufacturing:** Predictive maintenance to anticipate equipment failures.\n",
       "    *   **Real-life Example:** Factories use XGBoost to analyze sensor data from machinery and predict when a component is likely to fail, allowing for maintenance before a breakdown occurs, minimizing downtime.\n",
       "*   **Energy:** Predicting energy consumption or renewable energy generation.\n",
       "    *   **Real-life Example:** Power grid operators use XGBoost to forecast electricity demand and manage supply effectively.\n",
       "*   **Marketing:** Predicting customer response to marketing campaigns or identifying the most effective channels.\n",
       "    *   **Real-life Example:** Marketers use XGBoost to optimize ad spending and personalize marketing messages for better engagement.\n",
       "*   **Transportation:** Predicting traffic flow or optimizing logistics.\n",
       "    *   **Real-life Example:** Ride-sharing companies use XGBoost to predict demand in different areas and position drivers accordingly.\n",
       "\n",
       "## Conclusion:\n",
       "\n",
       "XGBoost's reputation as a \"Swiss Army knife\" for tabular data is well-deserved. Its combination of accuracy, speed, and robustness makes it an indispensable tool for data scientists and machine learning engineers across a vast array of industries. Whether it's predicting credit risk, detecting fraud, or forecasting sales, XGBoost consistently delivers powerful insights and actionable results."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experiment with 'candidate_count' parameter \n",
    "config = genai.types.GenerationConfig(candidate_count=1) \n",
    "result = get_response(\"Explain the concept of XGBoost with real-life use cases.\", generation_config=config) \n",
    "\n",
    "display(Markdown(result.text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c96251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a2aad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 700 \n",
    "CHUNK_OVERLAP = 100 \n",
    "pdf_path = \"http://jmc.stanford.edu/articles/whatisai/whatisai.pdf\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "835084ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(pdf_path) \n",
    "split_pdf_document = pdf_loader.load_and_split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a922c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP) \n",
    "context = \"\\n\\n\".join(str(p.page_content) for p in split_pdf_document) \n",
    "texts = text_splitter.split_text(context) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.5-flash-lite', \n",
    "    google_api_key='<YOUR_API_KEY>', \n",
    "    temperature=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805cda7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model='models/embedding-001', \n",
    "    google_api_key='<YOUR_API_KEY>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c17b8839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "vector_index = Chroma.from_texts(texts, embeddings) \n",
    "retriever = vector_index.as_retriever(search_kwargs={\"k\": 5}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73888a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    gemini_model, \n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84b0dcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the provided text, it's not explicitly stated who can learn AI. However, the text does mention that artificial intelligence (AI) is the \"science and engineering of making intelligent machines, especially intelligent computer programs.\" This suggests that individuals involved in computer science and engineering would be directly involved in the field.\n",
      "\n",
      "The text also mentions scientific societies and conferences related to AI research, such as the American Association for Artificial Intelligence (AAAI) and the International Joint Conference on AI (IJCAI). Participation in these organizations implies that researchers and academics in the field are actively learning and advancing AI.\n"
     ]
    }
   ],
   "source": [
    "# Example \n",
    "question = \"Who can learn AI?\" \n",
    "result = qa_chain.invoke({\"query\": question}) \n",
    "print(\"Answer:\", result[\"result\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f3c202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
