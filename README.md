# Build a RAG Using LangChain with Google Gemini

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/LangChain-Framework-orange)](https://www.langchain.com/)
[![Google Gemini](https://img.shields.io/badge/Google-Gemini-lightgrey?logo=google)](https://deepmind.google/technologies/gemini/)

---

## ðŸ“– Overview
This repository contains my walkthrough implementation of a **Retrieval-Augmented Generation (RAG)** pipeline using **LangChain** and **Google Gemini**.  

Although not a full production project, it showcases how Gemini can be connected with LangChain to create contextual responses, experiment with parameters, and build a simple RAG system on top of PDF data.

---

## ðŸš€ Tech Stack
- **Programming Language:** Python 3.10+
- **Framework:** [LangChain](https://www.langchain.com/)
- **LLM:** Google Gemini 2.5 Flash Lite
- **Libraries:**  
  - `langchain`  
  - `google-generativeai`  
  - `PyPDF2` (for text extraction)  

---

## ðŸ§  Key Learnings
- Building **LLM functionality** with LangChain to generate contextual responses.
- Experimenting with **model parameters** like `temperature`, `top_k`, `top_p`, and `candidate_count`.
- Implementing a **basic RAG pipeline** with PDF ingestion and embedding generation.
- Understanding Geminiâ€™s **chat history and response management**.

---

## ðŸ“Œ Notes

This project was a learning walkthrough and not a production-ready application.
It serves as a reference for using LangChain with Google Gemini in RAG workflows.DME.md # Pro
